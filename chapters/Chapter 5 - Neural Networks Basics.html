
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 5 - Neural Networks Basics &#8212; Machine Learning for Dummies</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=f0c89327" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=d2032c04" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=f0c89327" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/theme.css?v=a243ae73" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/Chapter 5 - Neural Networks Basics';</script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/searchtools.js?v=63a53a7d"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/language_data.js?v=d4673a71"></script>
    <script src="../_static/copybutton_funcs.js?v=776a791e"></script>
    <script src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/scripts/bootstrap.js?v=7583a70d"></script>
    <script src="../_static/scripts/fontawesome.js?v=9b125980"></script>
    <script src="../_static/scripts/pydata-sphinx-theme.js?v=f62441ba"></script>
    <link rel="icon" href="../_static/course-logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 6 - Deep Learning Tools" href="Chapter%206%20-%20Deep%20Learning%20Tools.html" />
    <link rel="prev" title="Chapter 4 - Unsupervised Learning" href="Chapter%204%20-%20Unsupervised%20Learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/course-logo.png" class="logo__image only-light" alt="Machine Learning for Dummies - Home"/>
    <img src="../_static/course-logo.png" class="logo__image only-dark pst-js-only" alt="Machine Learning for Dummies - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations of Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Chapter%201%20-%20Introduction%20to%20Machine%20Learning.html">Chapter 1: Introduction to Machine Learning</a></li>

<li class="toctree-l1"><a class="reference internal" href="Chapter%202%20-%20Data%20Fundamentals.html">Chapter 2 - Data Fundamentals</a></li>







<li class="toctree-l1"><a class="reference internal" href="Chapter%203%20-%20Supervised%20Learning.html">Chapter 3 - Supervised Learning</a></li>







<li class="toctree-l1"><a class="reference internal" href="Chapter%204%20-%20Unsupervised%20Learning.html">Chapter 4 - Unsupervised Learning</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Fundamentals</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 5 - Neural Networks Basics</a></li>

<li class="toctree-l1"><a class="reference internal" href="Chapter%206%20-%20Deep%20Learning%20Tools.html">Chapter 6 - Deep Learning Tools</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Chapter%207%20-%20Convolutional%20Neural%20Networks.html">Chapter 7 - Convolutional Neural Networks</a></li>

<li class="toctree-l1"><a class="reference internal" href="Chapter%208%20-%20Sequential%20Data%20and%20RNNs.html">Chapter 8 - Sequential Data and RNNs</a></li>

<li class="toctree-l1"><a class="reference internal" href="Chapter%209%20-%20Modern%20Deep%20Learning.html">Chapter 9 - Modern Deep Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/shreyashguptas/Machine-Learning-for-Dummies/blob/main/chapters/Chapter 5 - Neural Networks Basics.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/shreyashguptas/Machine-Learning-for-Dummies" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/shreyashguptas/Machine-Learning-for-Dummies/edit/main/chapters/Chapter 5 - Neural Networks Basics.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/shreyashguptas/Machine-Learning-for-Dummies/issues/new?title=Issue%20on%20page%20%2Fchapters/Chapter 5 - Neural Networks Basics.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/Chapter 5 - Neural Networks Basics.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 5 - Neural Networks Basics</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Chapter 5 - Neural Networks Basics</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-neural-networks">Artificial Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#structure-basics">Structure Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons-and-connections">Neurons and Connections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layers-explained">Layers Explained</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input-layer">Input Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-layers">Hidden Layers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output-layer">Output Layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-architecture">Network Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-networks">Feed-Forward Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#network-topology">Network Topology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-connections">Layer Connections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-and-bias">Weight and Bias</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-neural-networks-anns"><strong>Artificial Neural Networks (ANNs)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-neural-networks"><strong>Types of Neural Networks</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-layer-neural-network"><strong>1. Single-Layer Neural Network</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#real-life-analogy">🔍 <strong>Real-life analogy:</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-layer-neural-network"><strong>2. Multi-Layer Neural Network</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">🔍 <strong>Real-life analogy:</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-neural-networks-dnns"><strong>3. Deep Neural Networks (DNNs)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">🔍 <strong>Real-life analogy:</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-architectures"><strong>4. Common Architectures</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-feedforward-neural-network-fnn"><strong>a. Feedforward Neural Network (FNN)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-convolutional-neural-network-cnn"><strong>b. Convolutional Neural Network (CNN)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#c-recurrent-neural-network-rnn"><strong>c. Recurrent Neural Network (RNN)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#d-long-short-term-memory-lstm"><strong>d. Long Short-Term Memory (LSTM)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#e-generative-adversarial-network-gan"><strong>e. Generative Adversarial Network (GAN)</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary"><strong>Summary</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-functions">Common Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function"><strong>Sigmoid Function</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#relu-rectified-linear-unit"><strong>ReLU (Rectified Linear Unit)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh-hyperbolic-tangent-function"><strong>Tanh (Hyperbolic Tangent Function)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#leaky-relu"><strong>Leaky ReLU</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Chapter 5: Neural Networks Basics</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-criteria"><strong>Selection Criteria</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-disadvantages"><strong>Advantages/Disadvantages</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vanishing-gradient-problem"><strong>Vanishing Gradient Problem</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modern-solutions"><strong>Modern Solutions</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation"><strong>Implementation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input-processing"><strong>1. Input Processing</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output-range"><strong>2. Output Range</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-efficiency"><strong>3. Computational Efficiency</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#best-practices"><strong>4. Best Practices</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation"><strong>Forward Propagation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><strong>1. Input Processing</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-calculations"><strong>2. Layer Calculations</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#signal-flow"><strong>3. Signal Flow</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output-generation"><strong>4. Output Generation</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-propagation">Backward Propagation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#error-calculation"><strong>Error Calculation</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule"><strong>Chain Rule</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-computation"><strong>Gradient Computation</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-updates"><strong>Weight Updates</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-process"><strong>Learning Process</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#information-flow"><strong>1. Information Flow</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#error-distribution"><strong>2. Error Distribution</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-adjustment"><strong>3. Weight Adjustment</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-learning"><strong>4. Iterative Learning</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-neural-networks">Training Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-training-concepts">Basic Training Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-size"><strong>Batch Size</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#epochs"><strong>Epochs</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate"><strong>Learning Rate</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions"><strong>Loss Functions</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process">Training Process</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">- Data Preparation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#model-initialization">- Model Initialization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop">- Training Loop</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-steps">- Validation Steps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-challenges">Common Challenges</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting"><strong>Overfitting</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#underfitting"><strong>Underfitting</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-issues"><strong>Convergence Issues</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-management"><strong>Memory Management</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-techniques">Optimization Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-optimizers">Basic Optimizers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-batch-gradient-descent">Mini-batch Gradient Descent</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-schedules">Learning Rate Schedules</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-optimizers">Advanced Optimizers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#adam"><strong>Adam</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rmsprop"><strong>RMSprop</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#adagrad"><strong>AdaGrad</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum"><strong>Momentum</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Optimization Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-optimization">Hyperparameter Optimization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">Early Stopping</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">Regularization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-5-neural-networks-basics">
<h1>Chapter 5 - Neural Networks Basics<a class="headerlink" href="#chapter-5-neural-networks-basics" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<section id="artificial-neural-networks">
<h2>Artificial Neural Networks<a class="headerlink" href="#artificial-neural-networks" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<p>Think of a neural network like a massive game of telephone between smart friends, where each friend (neuron) listens to others, thinks about what they heard, and passes along their own conclusion.</p>
</section>
<section id="structure-basics">
<h2>Structure Basics<a class="headerlink" href="#structure-basics" title="Link to this heading">#</a></h2>
<section id="neurons-and-connections">
<h3>Neurons and Connections<a class="headerlink" href="#neurons-and-connections" title="Link to this heading">#</a></h3>
<p>Imagine a huge high school where students pass notes during class. Each student (neuron) receives notes (information) from multiple other students, makes sense of all these messages, and then passes their own note to others. Some students are really popular and get lots of notes (strong connections), while others might only get a few (weak connections).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Student Network:
   📝 → 👤 → 📝
   📝 → 👤 → 📝
   📝 → 👤 → 📝
</pre></div>
</div>
<p>Just like students sharing different pieces of gossip, neurons share different pieces of information, each adding their own “opinion” before passing it along.</p>
</section>
<section id="layers-explained">
<h3>Layers Explained<a class="headerlink" href="#layers-explained" title="Link to this heading">#</a></h3>
<p>Think of a restaurant kitchen during busy hours. Different stations work together to create the perfect meal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Order Flow:
[Order Station] → [Prep Station] → [Cooking Station] → [Plating Station]
    Layer 1          Layer 2          Layer 3            Layer 4
</pre></div>
</div>
<p>Each station (layer) has specific workers (neurons) who take input from the previous station, process it in their own way, and pass it to the next station. Just like how raw ingredients gradually become a finished dish, raw data transforms into meaningful output through these layers.</p>
<section id="input-layer">
<h4>Input Layer<a class="headerlink" href="#input-layer" title="Link to this heading">#</a></h4>
<p>The input layer is like your body’s senses taking in information about the world. Imagine standing in a bakery:</p>
<ul class="simple">
<li><p>Your eyes see the pastries (visual input)</p></li>
<li><p>Your nose smells the fresh bread (scent input)</p></li>
<li><p>Your ears hear the oven timer (audio input)</p></li>
<li><p>Your fingers feel the warmth (touch input)</p></li>
</ul>
<p>Each sense (input neuron) captures different aspects of the experience, just like how a neural network’s input layer captures different features of the data.</p>
</section>
<section id="hidden-layers">
<h4>Hidden Layers<a class="headerlink" href="#hidden-layers" title="Link to this heading">#</a></h4>
<p>Hidden layers are like the thought process in your brain when making a decision. When deciding what to cook for dinner, you might consider:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Decision</span> <span class="n">Process</span><span class="p">:</span>
<span class="p">[</span><span class="n">What</span><span class="s1">&#39;s in fridge?] → [What can I make?] → [What do I feel like?] → [Final choice]</span>
     <span class="n">Layer</span> <span class="mi">1</span>             <span class="n">Layer</span> <span class="mi">2</span>              <span class="n">Layer</span> <span class="mi">3</span>               <span class="n">Output</span>
</pre></div>
</div>
<p>Each hidden layer processes information in more complex ways, combining and transforming it like your brain combining different factors to make a decision.</p>
</section>
<section id="output-layer">
<h4>Output Layer<a class="headerlink" href="#output-layer" title="Link to this heading">#</a></h4>
<p>The output layer is like the final answer after all the thinking is done. Imagine a judge in a cooking competition who tastes many different aspects of a dish and finally gives:</p>
<ul class="simple">
<li><p>A single score (for regression problems)</p></li>
<li><p>A yes/no decision (for binary classification)</p></li>
<li><p>A category choice (for multi-class problems)</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Cooking Competition Result:
[Taste] [Presentation] [Creativity] → Final Score
    All inputs combined     →  Single Output
</pre></div>
</div>
<p>Remember: Neural networks work like a well-coordinated team:</p>
<ul class="simple">
<li><p>Input Layer: Like your senses gathering information</p></li>
<li><p>Hidden Layers: Like your brain processing thoughts</p></li>
<li><p>Output Layer: Like your final decision</p></li>
<li><p>Connections: Like the relationships between team members</p></li>
</ul>
<p>The beauty of neural networks is how they mimic our own decision-making process: taking in information, processing it through multiple stages of thought, and arriving at a conclusion. Just like how we learn from experience to make better decisions, neural networks learn from training to make better predictions.</p>
</section>
</section>
</section>
<section id="network-architecture">
<h2>Network Architecture<a class="headerlink" href="#network-architecture" title="Link to this heading">#</a></h2>
<p>Think of a neural network like a complex assembly line in a chocolate factory, where each station has a specific role in creating the perfect chocolate bar.</p>
<section id="feed-forward-networks">
<h3>Feed-Forward Networks<a class="headerlink" href="#feed-forward-networks" title="Link to this heading">#</a></h3>
<p>Imagine a waterfall flowing down a mountain, where the water can only flow in one direction. In feed-forward networks, information flows similarly - always moving forward, never backward. Like a school assembly line where students pass through different grades:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Input → First Grade → Second Grade → Third Grade → Graduate
(Start)  (Learning)   (Building)    (Refining)   (Result)
</pre></div>
</div>
<p>In a chocolate factory, this would be like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Cocoa Beans → Grinding → Mixing → Molding → Packaging
    ↓           ↓         ↓        ↓         ↓
Each station only receives from previous and sends to next
</pre></div>
</div>
</section>
<section id="network-topology">
<h3>Network Topology<a class="headerlink" href="#network-topology" title="Link to this heading">#</a></h3>
<p>Think of network topology like the blueprint of a shopping mall. Just as a mall has different floors (layers) with various shops (neurons) connected by escalators and walkways (connections), neural networks have their own architectural design.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Mall Layout:
    Food Court (Output Layer)
         ↑
    Shops Floor (Hidden Layer 2)
         ↑
  Boutiques Floor (Hidden Layer 1)
         ↑
    Entrance Hall (Input Layer)
</pre></div>
</div>
</section>
<section id="layer-connections">
<h3>Layer Connections<a class="headerlink" href="#layer-connections" title="Link to this heading">#</a></h3>
<p>Imagine a postal service system where letters (information) travel between cities (layers). Each city has multiple post offices (neurons), and each post office can send mail to every post office in the next city. In neural networks, these connections carry information forward, like an intricate web of delivery routes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>City A (Layer 1)    City B (Layer 2)
   📫 ----→ 📫
   ↘    ↗
   📫 ----→ 📫
</pre></div>
</div>
</section>
<section id="weight-and-bias">
<h3>Weight and Bias<a class="headerlink" href="#weight-and-bias" title="Link to this heading">#</a></h3>
<p>Think of weights and biases like a cooking recipe where:</p>
<p><strong>Weights</strong> are like the importance of each ingredient. Just as adding more chocolate makes a cake more chocolatey, stronger weights make certain connections more important. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Cookie</span> <span class="n">Recipe</span> <span class="n">Weights</span><span class="p">:</span>
<span class="n">Flour</span><span class="p">:</span> <span class="mi">3</span><span class="n">x</span> <span class="n">importance</span>
<span class="n">Sugar</span><span class="p">:</span> <span class="mi">2</span><span class="n">x</span> <span class="n">importance</span>
<span class="n">Butter</span><span class="p">:</span> <span class="mi">1</span><span class="n">x</span> <span class="n">importance</span>
</pre></div>
</div>
<p><strong>Bias</strong> is like the chef’s personal touch - that extra pinch of salt that’s always added regardless of other ingredients. In neural networks, bias helps make decisions even when inputs are minimal, like a experienced chef who knows to add a basic amount of seasoning even before tasting.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Chef&#39;s Decision Making:
Basic Recipe (Input × Weight)
      +
Personal Touch (Bias)
      ↓
Final Dish (Output)
</pre></div>
</div>
<p>Remember: Neural network architecture is like designing a complex factory where:</p>
<ul class="simple">
<li><p>Feed-forward networks ensure orderly flow (like assembly lines)</p></li>
<li><p>Topology determines the overall structure (like building floors)</p></li>
<li><p>Connections create pathways (like delivery routes)</p></li>
<li><p>Weights and biases fine-tune the process (like recipe adjustments)</p></li>
</ul>
<p>The goal is to create an efficient system that can:</p>
<ul class="simple">
<li><p>Process information in an organized way</p></li>
<li><p>Learn from examples</p></li>
<li><p>Make accurate predictions</p></li>
<li><p>Improve over time</p></li>
</ul>
<p>Just like a well-designed factory, a good neural network architecture helps transform raw inputs into meaningful outputs through a series of carefully structured processing steps!</p>
</section>
</section>
<section id="artificial-neural-networks-anns">
<h2><strong>Artificial Neural Networks (ANNs)</strong><a class="headerlink" href="#artificial-neural-networks-anns" title="Link to this heading">#</a></h2>
<p>Imagine you are teaching a dog to recognize different objects, like a ball, a bone, and a slipper. Each time the dog sees an object, it has to decide, “Is this a ball, a bone, or a slipper?” The dog learns over time by being corrected. This is, in essence, how an artificial neural network works, except instead of a dog, it’s a computer, and instead of objects, it works with numbers and patterns.</p>
<p>An Artificial Neural Network (ANN) is like a simplified version of the human brain. The brain has billions of neurons that are connected and constantly send signals to each other. In an ANN, “neurons” are represented by small units (nodes) that process information. Instead of neurons talking to each other with electrical signals, these nodes talk to each other using numbers.</p>
<p>Here’s a simple way to think of it:</p>
<ul class="simple">
<li><p><strong>Neurons</strong> are like people in an office.</p></li>
<li><p><strong>Connections</strong> between neurons are like phone calls between people.</p></li>
<li><p>When one person (a neuron) gets some information, they decide what to do with it and call another person (the next neuron) to give them the message.</p></li>
<li><p>Each person (neuron) might have a different job. Some only deal with specific kinds of information, like “Is it round like a ball?” or “Is it soft like a slipper?”</p></li>
<li><p>In the end, the last person (neuron) in the chain gives the final answer: “Yes, it’s a ball.”</p></li>
</ul>
<p>This process of information passing through a network is called <strong>forward propagation</strong>. As the network “learns” over time, it gets better at making the right calls — just like how the dog eventually learns to tell the difference between a ball, a bone, and a slipper.</p>
</section>
<hr class="docutils" />
<section id="types-of-neural-networks">
<h2><strong>Types of Neural Networks</strong><a class="headerlink" href="#types-of-neural-networks" title="Link to this heading">#</a></h2>
<p>Neural networks come in different shapes and sizes depending on the problem they are solving. Just like tools in a toolbox, you use different types of networks for different tasks.</p>
<hr class="docutils" />
<section id="single-layer-neural-network">
<h3><strong>1. Single-Layer Neural Network</strong><a class="headerlink" href="#single-layer-neural-network" title="Link to this heading">#</a></h3>
<p>This is the simplest kind of neural network, like a straight line of people passing a message down a chain.</p>
<section id="real-life-analogy">
<h4>🔍 <strong>Real-life analogy:</strong><a class="headerlink" href="#real-life-analogy" title="Link to this heading">#</a></h4>
<p>Imagine you’re playing the classic “telephone game” where people stand in a line, and the first person whispers a message to the next person. The message goes straight down the line until it reaches the last person, who says it out loud.</p>
<p>In a single-layer neural network:</p>
<ul class="simple">
<li><p><strong>There’s only one line of people (neurons).</strong></p></li>
<li><p>The first person (input) gives information, and it’s passed directly to the last person (output).</p></li>
<li><p>No one in the middle has any special role except to pass the message.</p></li>
</ul>
<p>This type of network works well for simple tasks but struggles with anything complex. For example, if someone in the line mishears the message, there’s no way to fix it — it just goes through as is.</p>
</section>
</section>
<hr class="docutils" />
<section id="multi-layer-neural-network">
<h3><strong>2. Multi-Layer Neural Network</strong><a class="headerlink" href="#multi-layer-neural-network" title="Link to this heading">#</a></h3>
<p>This is like multiple layers of people playing the telephone game, with extra people in the middle to “double-check” the message.</p>
<section id="id1">
<h4>🔍 <strong>Real-life analogy:</strong><a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>Imagine instead of one straight line of people, you have <strong>two rows of people</strong>. The first row of people listens to the message and checks it. If it sounds weird, they correct it before passing it to the second row. The second row then passes it to the final person, who announces the message.</p>
<p>In a multi-layer neural network:</p>
<ul class="simple">
<li><p>There are multiple layers of “people” (neurons) checking and processing the message.</p></li>
<li><p>The first layer breaks down the information into smaller pieces (like “Is it round?” “Is it red?”).</p></li>
<li><p>The second layer combines these smaller pieces to figure out the final answer.</p></li>
<li><p>If something goes wrong in one layer, the next layer can correct it.</p></li>
</ul>
<p>This type of network can solve more complex problems like recognizing handwriting or voice commands because it “double-checks” the message at each stage.</p>
</section>
</section>
<hr class="docutils" />
<section id="deep-neural-networks-dnns">
<h3><strong>3. Deep Neural Networks (DNNs)</strong><a class="headerlink" href="#deep-neural-networks-dnns" title="Link to this heading">#</a></h3>
<p>This is like the multi-layer network but with <strong>many, many layers</strong> of people checking and re-checking information.</p>
<section id="id2">
<h4>🔍 <strong>Real-life analogy:</strong><a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<p>Imagine you’re trying to identify a rare type of bird, and you have to pass through many levels of “bird experts” to get the answer.</p>
<ol class="arabic simple">
<li><p>The first expert might just check if it’s a bird at all.</p></li>
<li><p>The second expert checks if it has a beak or feathers.</p></li>
<li><p>The third expert narrows it down to specific types of birds.</p></li>
<li><p>The process continues until you finally reach a “master bird expert” who can name the exact species.</p></li>
</ol>
<p>In a deep neural network:</p>
<ul class="simple">
<li><p>Each layer of neurons is like a different level of experts with specialized knowledge.</p></li>
<li><p>Early layers do simple checks, like “Is it round? Does it have fur?”</p></li>
<li><p>Later layers combine all this information to make a final decision, like “It’s a dog, not a cat.”</p></li>
<li><p>The deeper (more layers) the network, the better it can recognize complex things like faces, voices, and objects in photos.</p></li>
</ul>
<p>Deep neural networks are used in apps like <strong>facial recognition</strong>, <strong>self-driving cars</strong>, and <strong>voice assistants (like Siri or Alexa)</strong>.</p>
</section>
</section>
<hr class="docutils" />
<section id="common-architectures">
<h3><strong>4. Common Architectures</strong><a class="headerlink" href="#common-architectures" title="Link to this heading">#</a></h3>
<p>Different types of neural networks are used for different problems, just like how you wouldn’t use a hammer to fix a computer. Here are a few important ones:</p>
<section id="a-feedforward-neural-network-fnn">
<h4><strong>a. Feedforward Neural Network (FNN)</strong><a class="headerlink" href="#a-feedforward-neural-network-fnn" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>🔍 <strong>Analogy</strong>: Like a one-way street. Information flows in one direction, from input to output.</p></li>
<li><p>Used for: Predicting things like house prices, stock prices, or weather forecasts.</p></li>
</ul>
</section>
<section id="b-convolutional-neural-network-cnn">
<h4><strong>b. Convolutional Neural Network (CNN)</strong><a class="headerlink" href="#b-convolutional-neural-network-cnn" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>🔍 <strong>Analogy</strong>: Imagine you have a scanner that moves across a photo, checking for specific patterns (like edges or colors) to figure out what’s in the picture.</p></li>
<li><p>Used for: Image recognition, like how your phone detects faces in photos.</p></li>
</ul>
</section>
<section id="c-recurrent-neural-network-rnn">
<h4><strong>c. Recurrent Neural Network (RNN)</strong><a class="headerlink" href="#c-recurrent-neural-network-rnn" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>🔍 <strong>Analogy</strong>: Like a person with a notebook who writes down what happened in the past so they can remember it later.</p></li>
<li><p>Used for: Predicting the next word in a sentence (like auto-suggestions in texting), or for processing sequences of data like video or audio.</p></li>
</ul>
</section>
<section id="d-long-short-term-memory-lstm">
<h4><strong>d. Long Short-Term Memory (LSTM)</strong><a class="headerlink" href="#d-long-short-term-memory-lstm" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>🔍 <strong>Analogy</strong>: Imagine someone with a “super-memory” who remembers important details but forgets unimportant ones.</p></li>
<li><p>Used for: Things like language translation and predicting stock prices where “remembering” past data is essential.</p></li>
</ul>
</section>
<section id="e-generative-adversarial-network-gan">
<h4><strong>e. Generative Adversarial Network (GAN)</strong><a class="headerlink" href="#e-generative-adversarial-network-gan" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>🔍 <strong>Analogy</strong>: Imagine two artists. One artist draws a fake painting, and the other is a critic who tries to tell if it’s real or fake. The first artist gets better at drawing realistic paintings to fool the critic.</p></li>
<li><p>Used for: Generating realistic images, deepfakes, or even creating new music.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2><strong>Summary</strong><a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Artificial Neural Networks</strong> are like human brains but made of numbers and math.</p></li>
<li><p>They have neurons that pass information from one to another, similar to people in a telephone game.</p></li>
<li><p><strong>Types of Neural Networks</strong> range from simple single-layer systems to more advanced deep networks with many layers.</p></li>
<li><p>Each network has a specific purpose, like recognizing faces (CNNs), predicting the next word in a sentence (RNNs), or generating new content like images (GANs).</p></li>
</ol>
<p>By thinking of these concepts as people in a chain, expert birdwatchers, and game-like scenarios, you can begin to understand how neural networks work.</p>
</section>
<section id="activation-functions">
<h2>Activation Functions<a class="headerlink" href="#activation-functions" title="Link to this heading">#</a></h2>
<p>When we think of neural networks, it’s helpful to imagine them as decision-makers. But just like people, these decision-makers need a way to determine how “strongly” they should react to certain inputs. Activation functions are like switches or dials that control how much of a reaction a neuron should have when it receives information.</p>
<p>Here’s an analogy: Imagine a faucet with water flowing through it. The handle of the faucet determines how much water flows out. If you barely turn it, only a little water comes out. If you turn it all the way, you get a strong, steady flow. Activation functions are like that faucet handle — they control how “open” the neuron is to letting information flow through.</p>
<hr class="docutils" />
<section id="common-functions">
<h3>Common Functions<a class="headerlink" href="#common-functions" title="Link to this heading">#</a></h3>
<section id="sigmoid-function">
<h4><strong>Sigmoid Function</strong><a class="headerlink" href="#sigmoid-function" title="Link to this heading">#</a></h4>
<p><strong>Analogy</strong>: Imagine a dimmer switch for lights in your living room. At first, when you turn the dial, the lights brighten very slowly. As you turn it more, the brightness increases faster. But once the light gets really bright, turning the dial more doesn’t have much of an effect.</p>
<p><strong>Explanation</strong>: The sigmoid function works similarly. It takes any input (positive or negative) and squashes it into a range between 0 and 1. Small inputs (like dimming a light) result in small outputs, and large inputs start to “level off” near 1 (like the light being at full brightness). This makes it great for situations where you want to classify something as “yes” (1) or “no” (0), like deciding if an email is spam or not.</p>
</section>
<hr class="docutils" />
<section id="relu-rectified-linear-unit">
<h4><strong>ReLU (Rectified Linear Unit)</strong><a class="headerlink" href="#relu-rectified-linear-unit" title="Link to this heading">#</a></h4>
<p><strong>Analogy</strong>: Picture a door with a spring. If you try to push the door from the wrong side, it won’t budge at all — it just stays closed. But if you push from the correct side, the door swings open freely.</p>
<p><strong>Explanation</strong>: ReLU acts like that door. If the input is negative (pushing from the wrong side), the output is 0 (the door stays shut). But if the input is positive (pushing from the right side), the output is exactly the same as the input (the door swings open just as far as you push it). This simplicity makes ReLU one of the most widely used activation functions in deep learning.</p>
</section>
<hr class="docutils" />
<section id="tanh-hyperbolic-tangent-function">
<h4><strong>Tanh (Hyperbolic Tangent Function)</strong><a class="headerlink" href="#tanh-hyperbolic-tangent-function" title="Link to this heading">#</a></h4>
<p><strong>Analogy</strong>: Think of a thermostat in a house that controls both heating and cooling. If the temperature is too cold, the heater turns on (positive output). If it’s too hot, the air conditioner turns on (negative output). But when the temperature is just right, neither system is running (output near zero).</p>
<p><strong>Explanation</strong>: The tanh function takes inputs and transforms them into a range between -1 and 1. If the input is strongly negative, the output will be close to -1 (like the air conditioner working hard). If the input is strongly positive, the output will be close to 1 (like the heater working hard). When the input is close to zero, the output is also close to zero, similar to when the house temperature is just right.</p>
</section>
<hr class="docutils" />
<section id="leaky-relu">
<h4><strong>Leaky ReLU</strong><a class="headerlink" href="#leaky-relu" title="Link to this heading">#</a></h4>
<p><strong>Analogy</strong>: Imagine a door that’s supposed to stay shut when you push on it from the wrong side. But unlike the earlier “spring door” (ReLU), this door has a tiny crack at the bottom that still lets a little air through.</p>
<p><strong>Explanation</strong>: Leaky ReLU works like that cracked door. If the input is negative, instead of being completely blocked at zero, it allows a small, “leaky” output (a small negative number) to pass through. If the input is positive, it behaves just like ReLU — the output is the same as the input. This small “leak” helps prevent a problem where too many neurons get stuck producing zero outputs, which is known as the “dying ReLU” problem.</p>
</section>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id3">
<h1>Chapter 5: Neural Networks Basics<a class="headerlink" href="#id3" title="Link to this heading">#</a></h1>
<section id="id4">
<h2>Activation Functions<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p>Activation functions are like “decision-makers” for a neural network. Imagine you have a friend who gives you advice on whether you should do something or not. Depending on the situation, they might say “yes,” “no,” or “maybe.” Similarly, activation functions decide whether the information coming into a neuron should be “activated” (passed forward) or “ignored” (held back) as it moves through the neural network.</p>
<hr class="docutils" />
<section id="selection-criteria">
<h3><strong>Selection Criteria</strong><a class="headerlink" href="#selection-criteria" title="Link to this heading">#</a></h3>
<p>When deciding which activation function to use, it’s like choosing the right type of coach for a sports team. Each coach has a unique style, and you pick one based on the team’s needs.</p>
<ul class="simple">
<li><p><strong>ReLU (Rectified Linear Unit)</strong></p>
<ul>
<li><p><strong>When to Use:</strong> Ideal for deep neural networks where speed is crucial.</p></li>
<li><p><strong>Why:</strong> ReLU only activates (passes) positive values, while zeroing out negative values.</p></li>
<li><p><strong>Real-life Analogy:</strong> Imagine a security guard checking IDs at a club. If you’re underage (negative input), you’re denied entry (output = 0). If you’re of age (positive input), you’re allowed in (output = your age). This speeds up the process because the guard doesn’t spend time debating — it’s a quick yes or no.</p></li>
<li><p><strong>Advantages:</strong> Simple, fast, and effective for deep networks.</p></li>
<li><p><strong>Disadvantages:</strong> Can “ignore” too much data if too many inputs are negative, leading to “dead neurons” that never activate again.</p></li>
</ul>
</li>
<li><p><strong>Sigmoid</strong></p>
<ul>
<li><p><strong>When to Use:</strong> Good for models where you need probabilities (like classification) since it outputs values between 0 and 1.</p></li>
<li><p><strong>Why:</strong> The sigmoid “squishes” all input values into a smooth curve between 0 and 1.</p></li>
<li><p><strong>Real-life Analogy:</strong> Imagine a dimmer switch for a light bulb. Turn it all the way down (negative input), and the light is off (output close to 0). Turn it all the way up (positive input), and the light is fully on (output close to 1). Inputs in between result in partial brightness.</p></li>
<li><p><strong>Advantages:</strong> Useful when predicting probabilities.</p></li>
<li><p><strong>Disadvantages:</strong> If inputs are too large or too small, the changes become insignificant, like turning a dimmer switch but barely seeing any change.</p></li>
</ul>
</li>
<li><p><strong>Tanh (Hyperbolic Tangent)</strong></p>
<ul>
<li><p><strong>When to Use:</strong> When you want both positive and negative outputs, not just 0 to 1 like sigmoid.</p></li>
<li><p><strong>Why:</strong> It squashes input to a range between -1 and 1.</p></li>
<li><p><strong>Real-life Analogy:</strong> Think of a thermometer. If it’s extremely cold (strong negative input), it shows a very low number (close to -1). If it’s extremely hot (strong positive input), it shows a high number (close to +1). Mild temperatures fall somewhere in between.</p></li>
<li><p><strong>Advantages:</strong> Outputs are centered around 0, which makes training easier compared to sigmoid.</p></li>
<li><p><strong>Disadvantages:</strong> Like the sigmoid, it can also get “stuck” when inputs are too large or too small, making learning slow.</p></li>
</ul>
</li>
<li><p><strong>Leaky ReLU</strong></p>
<ul>
<li><p><strong>When to Use:</strong> When you want the benefits of ReLU but don’t want to “kill” neurons completely.</p></li>
<li><p><strong>Why:</strong> Unlike ReLU, which ignores negative inputs, Leaky ReLU allows small negative outputs instead of zero.</p></li>
<li><p><strong>Real-life Analogy:</strong> Imagine a faucet that leaks water. Instead of being fully closed (like ReLU), it lets a little water (small negative value) trickle out, even when it’s off.</p></li>
<li><p><strong>Advantages:</strong> Prevents “dead neurons” that can’t recover.</p></li>
<li><p><strong>Disadvantages:</strong> Can still face issues with some data not being useful.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="advantages-disadvantages">
<h3><strong>Advantages/Disadvantages</strong><a class="headerlink" href="#advantages-disadvantages" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Activation Function</strong></p></th>
<th class="head"><p><strong>Speed</strong></p></th>
<th class="head"><p><strong>Works Well With Deep Networks?</strong></p></th>
<th class="head"><p><strong>Handles Negatives?</strong></p></th>
<th class="head"><p><strong>Output Range</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>ReLU</strong></p></td>
<td><p>⭐⭐⭐ Very fast</p></td>
<td><p>✅ Yes</p></td>
<td><p>❌ No</p></td>
<td><p>0 to ∞ (only positive)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Sigmoid</strong></p></td>
<td><p>⭐ Slower</p></td>
<td><p>❌ No</p></td>
<td><p>❌ No</p></td>
<td><p>0 to 1 (like probability)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Tanh</strong></p></td>
<td><p>⭐⭐ Medium</p></td>
<td><p>❌ No</p></td>
<td><p>✅ Yes</p></td>
<td><p>-1 to 1</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Leaky ReLU</strong></p></td>
<td><p>⭐⭐⭐ Very fast</p></td>
<td><p>✅ Yes</p></td>
<td><p>✅ Yes</p></td>
<td><p>Negative to ∞ (small leak)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="vanishing-gradient-problem">
<h3><strong>Vanishing Gradient Problem</strong><a class="headerlink" href="#vanishing-gradient-problem" title="Link to this heading">#</a></h3>
<p>The “vanishing gradient problem” is a major issue in training neural networks. Here’s a simple way to think about it:</p>
<p><strong>Real-life Analogy:</strong>
Imagine you’re playing the “telephone game” with friends. You whisper a message to the first friend, they pass it on to the second, and so on. If each friend speaks too softly (like small gradients), by the time the message reaches the last person, it’s barely audible or completely lost.</p>
<p>This happens in neural networks when small changes to the weights of neurons shrink to almost nothing as they move backward through layers. When training, the earlier layers never “hear” the important feedback, so they don’t learn properly.</p>
<p><strong>Which Activation Functions Suffer From This?</strong></p>
<ul class="simple">
<li><p><strong>Sigmoid</strong>: Since sigmoid “squashes” values to 0 or 1, tiny changes in input make almost no change in the output for large positive or negative inputs.</p></li>
<li><p><strong>Tanh</strong>: Tanh has the same issue as sigmoid, but since its range is from -1 to 1, it’s a bit better.</p></li>
<li><p><strong>ReLU/Leaky ReLU</strong>: Since ReLU doesn’t squash large inputs, it avoids the vanishing gradient problem. This is why ReLU is so popular in modern deep learning.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="modern-solutions">
<h3><strong>Modern Solutions</strong><a class="headerlink" href="#modern-solutions" title="Link to this heading">#</a></h3>
<p>Over time, smart methods have been developed to solve the vanishing gradient problem.</p>
<ol class="arabic simple">
<li><p><strong>ReLU and Variations (like Leaky ReLU)</strong></p>
<ul class="simple">
<li><p><strong>How It Helps:</strong> Since ReLU doesn’t squash large inputs, gradients can “flow” back through the network, like using a loudspeaker in the telephone game so the first friend hears the message clearly.</p></li>
<li><p><strong>Real-life Analogy:</strong> Imagine shouting your message instead of whispering it. The message is much more likely to be heard clearly.</p></li>
</ul>
</li>
<li><p><strong>Batch Normalization</strong></p>
<ul class="simple">
<li><p><strong>How It Helps:</strong> Normalizes (scales) input data before it enters the next layer, ensuring it stays at a reasonable size.</p></li>
<li><p><strong>Real-life Analogy:</strong> If you’re baking bread, the yeast (neuron weights) can only rise properly if the water temperature is “just right.” Batch normalization keeps the temperature (data range) consistent, so the bread rises properly at every step.</p></li>
</ul>
</li>
<li><p><strong>Better Weight Initialization</strong></p>
<ul class="simple">
<li><p><strong>How It Helps:</strong> If weights are initialized too large or small, they can cause vanishing/exploding gradients. Better methods (like He initialization) set the starting point just right.</p></li>
<li><p><strong>Real-life Analogy:</strong> When you’re learning to juggle, starting with 3 lightweight balls (appropriate weight initialization) is much easier than starting with 3 bowling balls (poor weight initialization). You’re more likely to succeed.</p></li>
</ul>
</li>
<li><p><strong>Gradient Clipping</strong></p>
<ul class="simple">
<li><p><strong>How It Helps:</strong> If gradients get too large, they “clip” them to a maximum size, like putting a speed limit on a highway to avoid reckless driving.</p></li>
<li><p><strong>Real-life Analogy:</strong> If you’re driving on a road with a strict 30 mph speed limit (clipping), even if you press the gas hard, you can’t go faster than 30 mph. This prevents “exploding gradients” where everything goes out of control.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="id5">
<h2>Activation Functions<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>Activation functions are one of the most important parts of a neural network. Think of them as decision-makers that help the network figure out what to focus on. Without activation functions, a neural network would just be a fancy form of basic addition and multiplication.</p>
<p>Imagine you’re running a small bakery. Each day, you get a variety of ingredients (inputs) like flour, sugar, eggs, and butter. To make a cake (the output), you need to process these ingredients in a specific way — mix, bake, and decorate. The “activation function” is like the recipe step that decides whether a particular mixture should go to the oven or be discarded.</p>
<hr class="docutils" />
<section id="implementation">
<h3><strong>Implementation</strong><a class="headerlink" href="#implementation" title="Link to this heading">#</a></h3>
<section id="input-processing">
<h4><strong>1. Input Processing</strong><a class="headerlink" href="#input-processing" title="Link to this heading">#</a></h4>
<p>Think of input processing as sorting through ingredients at the start of your baking process. You have raw materials (inputs) that need to be processed in a way that makes sense for the recipe.</p>
<p>In a neural network, these inputs are numbers, like how much sugar or flour you have. The activation function takes these “ingredients” and processes them to decide if they are useful for the “cake” (output) or not. If you were baking, this would be like deciding if you have enough flour to make a cake — if you only have 1 teaspoon of flour, you might ignore it, but if you have 2 cups, you use it.</p>
<p>In a neural network, an input might be too small or too large to be useful, and the activation function helps “filter” what should be passed on to the next stage.</p>
</section>
<hr class="docutils" />
<section id="output-range">
<h4><strong>2. Output Range</strong><a class="headerlink" href="#output-range" title="Link to this heading">#</a></h4>
<p>Once the activation function processes the input, it produces an output. This output has a specific range. Think of this like baking different types of cakes. Some recipes (like sponge cakes) can only be made if the oven is set to a specific temperature range (say 325°F to 375°F). If the temperature is outside this range, the cake won’t bake properly.</p>
<p>Similarly, activation functions “limit” the output to a certain range. For example:</p>
<ul class="simple">
<li><p><strong>Some functions keep the output between 0 and 1</strong> — like having an on/off switch.</p></li>
<li><p><strong>Others keep it between -1 and 1</strong> — like a thermostat that can heat or cool.</p></li>
<li><p><strong>Some don’t limit it at all</strong> — like an oven with no temperature limit, but this can be risky because too high or too low might not give good results.</p></li>
</ul>
<p>This “range control” helps the neural network stay stable and make decisions consistently. Imagine if you tried to bake at 1000°F — your cake would be ruined. Similarly, if the output of a neural network is too large or too small, it can throw off the entire learning process.</p>
</section>
<hr class="docutils" />
<section id="computational-efficiency">
<h4><strong>3. Computational Efficiency</strong><a class="headerlink" href="#computational-efficiency" title="Link to this heading">#</a></h4>
<p>Efficiency is about how quickly and easily something can be done. Imagine you’re a baker and you’re trying to whip egg whites into a fluffy foam. If you do it by hand, it takes forever. But if you use an electric mixer, it’s much faster.</p>
<p>In a neural network, computational efficiency is like using an electric mixer. Some activation functions are simple to compute (like flipping a light switch on and off), while others are more complicated (like whisking egg whites by hand). The simpler the function, the faster the whole system can work.</p>
<p>For example, a simple “on/off” switch activation function is much faster than one that requires calculating complex curves. This matters because, in large neural networks, a slow function can make the entire process take hours or days longer.</p>
</section>
<hr class="docutils" />
<section id="best-practices">
<h4><strong>4. Best Practices</strong><a class="headerlink" href="#best-practices" title="Link to this heading">#</a></h4>
<p>Finally, let’s talk about best practices — the rules that help bakers avoid common mistakes. If you’ve ever forgotten to preheat your oven, you know how frustrating it is to wait. Similarly, using the wrong activation function can slow down training or make the neural network perform poorly.</p>
<p>Some “best practices” for activation functions are:</p>
<ul class="simple">
<li><p><strong>Pick the right range</strong>: Just like some cakes require a specific oven temperature range, some neural networks work best with specific activation functions.</p></li>
<li><p><strong>Keep it simple</strong>: If a simple switch works (like ReLU, which just turns off negative numbers), use it.</p></li>
<li><p><strong>Avoid extreme values</strong>: Just like a cake can burn if the oven is too hot, avoid activation functions that create extremely large or small numbers.</p></li>
<li><p><strong>Use standard recipes</strong>: Popular functions like ReLU, Sigmoid, and Tanh are “proven recipes” that work for most cases.</p></li>
</ul>
<hr class="docutils" />
<p>By understanding these concepts, you’ll see how activation functions play a crucial role in “baking” better neural networks. They decide which inputs are useful, control the size of the output, keep the process efficient, and help you follow best practices for success.</p>
</section>
</section>
<section id="forward-propagation">
<h3><strong>Forward Propagation</strong><a class="headerlink" href="#forward-propagation" title="Link to this heading">#</a></h3>
<p>Forward propagation is like how information flows through a system step-by-step. To understand this, let’s think of it as a “decision-making machine” like ordering food at a restaurant.</p>
<hr class="docutils" />
<section id="id6">
<h4><strong>1. Input Processing</strong><a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<p>Imagine you walk into a restaurant, and you’re given a menu. The “menu” represents the inputs to a neural network. Each item on the menu is like a piece of information that the system needs to process.</p>
<ul class="simple">
<li><p><strong>Real-life analogy:</strong> You’re hungry, and you see a menu with options like pizza, pasta, and salad. Your brain takes in this information (the inputs) and starts thinking about what you want to eat.</p></li>
<li><p><strong>Neural network analogy:</strong> The network takes in raw data (like an image, sound, or text) and starts organizing it to prepare for decision-making. This input data could be numbers, text, or pixels from an image.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="layer-calculations">
<h4><strong>2. Layer Calculations</strong><a class="headerlink" href="#layer-calculations" title="Link to this heading">#</a></h4>
<p>Once you have the menu (input), you start to consider your choices. Each option on the menu has “ingredients” (like cheese, tomatoes, etc.), and you think about which option matches your preferences. This step is like how layers in a neural network process and transform the input.</p>
<ul class="simple">
<li><p><strong>Real-life analogy:</strong> You see that the pizza has cheese, sauce, and toppings, but you’re lactose intolerant. Your brain “processes” this information and eliminates the pizza as an option. Your mind is doing calculations to decide which meal works for you.</p></li>
<li><p><strong>Neural network analogy:</strong> Each layer in a neural network applies “rules” (like weighing the importance of cheese, sauce, and toppings) to the input. It does some math (like weighing and activating) to figure out which choices to keep and which to ignore.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="signal-flow">
<h4><strong>3. Signal Flow</strong><a class="headerlink" href="#signal-flow" title="Link to this heading">#</a></h4>
<p>Now that you’ve narrowed down your options, you pass this decision on to your taste preferences. You send the “signal” through your brain — “I want something light, so I’ll pick the salad.” This flow of logic is how information moves from one step to another.</p>
<ul class="simple">
<li><p><strong>Real-life analogy:</strong> Your brain sends a signal — “Pick the salad!” — and this message gets stronger as you consider your health goals and hunger level.</p></li>
<li><p><strong>Neural network analogy:</strong> The output from one layer becomes the input for the next layer. The “signal” moves from one part of the network to the next, getting refined at each step, just like your choice of food becomes clearer as you think it through.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="output-generation">
<h4><strong>4. Output Generation</strong><a class="headerlink" href="#output-generation" title="Link to this heading">#</a></h4>
<p>Finally, you tell the waiter, “I want the salad.” This is the final decision, which is the “output” of the process.</p>
<ul class="simple">
<li><p><strong>Real-life analogy:</strong> After all the thinking, you choose your meal (the salad) and tell the waiter your choice. This is the final result of the whole process.</p></li>
<li><p><strong>Neural network analogy:</strong> After all the layers process the input, the network produces an answer or prediction. If it’s a self-driving car, the output might be “Turn left.” If it’s a spam filter, the output might be “This email is spam.” This is the end of the forward propagation process — you get a decision or prediction.</p></li>
</ul>
<hr class="docutils" />
<p>This is the step-by-step journey of how information flows through a neural network. It’s like deciding on your meal at a restaurant: look at the menu (input), consider your options (layer calculations), send the signal (signal flow), and finally, place your order (output).</p>
</section>
</section>
<section id="backward-propagation">
<h3>Backward Propagation<a class="headerlink" href="#backward-propagation" title="Link to this heading">#</a></h3>
<hr class="docutils" />
<section id="error-calculation">
<h4><strong>Error Calculation</strong><a class="headerlink" href="#error-calculation" title="Link to this heading">#</a></h4>
<p>Imagine you are a teacher grading a student’s math test. The student attempted a problem and got an answer of <strong>7</strong>, but the correct answer was <strong>10</strong>. The difference between the student’s answer and the correct answer is the <strong>error</strong>. In neural networks, this “difference” tells the network how far off its predictions are from the actual, correct result.</p>
<p>In simpler terms, the error is like a “score” that tells the network, “Hey, you were off by this much!” The goal is to make this score as small as possible, just like a teacher wants students to have fewer mistakes on their tests.</p>
</section>
<hr class="docutils" />
<section id="chain-rule">
<h4><strong>Chain Rule</strong><a class="headerlink" href="#chain-rule" title="Link to this heading">#</a></h4>
<p>Let’s say you’re on a road trip, and you need to calculate how long it will take to get to your destination. But here’s the catch — the total time depends on multiple factors:</p>
<ol class="arabic simple">
<li><p><strong>Speed</strong> of the car</p></li>
<li><p><strong>Traffic conditions</strong></p></li>
<li><p><strong>Weather</strong></p></li>
</ol>
<p>Each of these factors affects the total time in a small way. The chain rule is like understanding how a small change in one factor (like speed) influences the overall trip time.</p>
<p>In neural networks, we have many “factors” (like neurons and layers) that affect the final result. The chain rule helps us figure out how changing one small thing in an early part of the system affects everything that comes after it. It’s like saying, “If I drive a little faster, how much sooner will I reach my destination?”</p>
</section>
<hr class="docutils" />
<section id="gradient-computation">
<h4><strong>Gradient Computation</strong><a class="headerlink" href="#gradient-computation" title="Link to this heading">#</a></h4>
<p>Imagine you’re hiking up a mountain, and you want to find the fastest way to get to the top. You could wander aimlessly, or you could check which direction is the steepest uphill climb and head that way. That steepness is called the <strong>gradient</strong>.</p>
<p>In neural networks, the gradient tells us the direction to move in to make the error smaller. Instead of hiking to the top of a mountain, the network is trying to “climb” toward a better prediction. If the slope is steep, it means there’s a big opportunity to improve. If the slope is flat, you’re close to the top (or the best prediction).</p>
</section>
<hr class="docutils" />
<section id="weight-updates">
<h4><strong>Weight Updates</strong><a class="headerlink" href="#weight-updates" title="Link to this heading">#</a></h4>
<p>Imagine you’re trying to learn how to shoot a basketball. On your first shot, you aim too far to the right. So, on your next shot, you aim more to the left. You keep adjusting your aim with each shot, using what you learned from the previous shot.</p>
<p>In neural networks, “weights” are like your aim. After the network sees how big the error is (like missing the basketball shot), it adjusts its “aim” (the weights) to do better next time. The amount of adjustment depends on the gradient (how steep the slope is) and how far off the network was. Over time, these small adjustments help the network “learn” to predict better.</p>
<hr class="docutils" />
<p>These concepts work together to help the neural network get smarter over time. It checks its mistakes (Error Calculation), figures out how each part of the system contributed to the mistake (Chain Rule), measures which way to move to improve (Gradient Computation), and then actually makes adjustments (Weight Updates) to get closer to the right answer.</p>
</section>
</section>
<section id="learning-process">
<h3><strong>Learning Process</strong><a class="headerlink" href="#learning-process" title="Link to this heading">#</a></h3>
<hr class="docutils" />
<section id="information-flow">
<h4><strong>1. Information Flow</strong><a class="headerlink" href="#information-flow" title="Link to this heading">#</a></h4>
<p>Imagine you are part of a team building a cake using a recipe. Each person in the team has a specific role: one person measures ingredients, another mixes them, and someone else bakes the cake. Each person passes their output (like mixed batter) to the next person.</p>
<p>In a neural network, information flows similarly. Each layer of the network takes inputs (like ingredients), processes them (like mixing or stirring), and passes the result (like the batter) to the next layer. This step-by-step movement of information from input to output is called <strong>forward propagation</strong>.</p>
</section>
<section id="error-distribution">
<h4><strong>2. Error Distribution</strong><a class="headerlink" href="#error-distribution" title="Link to this heading">#</a></h4>
<p>Now, suppose the cake doesn’t taste right — maybe it’s too salty. Everyone in the team must figure out where things went wrong. Did the ingredient measurer add too much salt? Did the mixer blend it incorrectly? Or did the baker overcook it?</p>
<p>In a neural network, if the output (like a prediction) is wrong, we need to “blame” each layer to see where the mistake happened. This process is called <strong>error distribution</strong> or <strong>backpropagation of error</strong>. Each layer gets feedback about how much it contributed to the mistake.</p>
</section>
<section id="weight-adjustment">
<h4><strong>3. Weight Adjustment</strong><a class="headerlink" href="#weight-adjustment" title="Link to this heading">#</a></h4>
<p>Let’s say the problem was that the ingredient measurer added too much salt. To prevent this from happening again, the team decides to change the way they measure salt. Instead of using a big spoon, they use a smaller one.</p>
<p>In a neural network, “weights” are like the measuring tools. If the network realizes that a specific “ingredient” (input) was too strong or too weak, it adjusts the “weight” (the size of the spoon) so the error is less likely to happen again. This process is called <strong>weight adjustment</strong>.</p>
</section>
<section id="iterative-learning">
<h4><strong>4. Iterative Learning</strong><a class="headerlink" href="#iterative-learning" title="Link to this heading">#</a></h4>
<p>The team doesn’t just bake one cake and call it a day. They bake many cakes, improving each time. After every cake, they taste it, analyze what went wrong, adjust their process, and try again.</p>
<p>Neural networks also learn this way. They don’t learn everything at once. Instead, they go through many rounds of forward propagation, error distribution, and weight adjustment. This cycle happens over and over, getting closer to perfection with each “iteration.” This is why it’s called <strong>iterative learning</strong>.</p>
</section>
</section>
</section>
<section id="training-neural-networks">
<h2>Training Neural Networks<a class="headerlink" href="#training-neural-networks" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<section id="basic-training-concepts">
<h3>Basic Training Concepts<a class="headerlink" href="#basic-training-concepts" title="Link to this heading">#</a></h3>
<section id="batch-size">
<h4><strong>Batch Size</strong><a class="headerlink" href="#batch-size" title="Link to this heading">#</a></h4>
<p>Imagine you are trying to learn how to bake cookies. You have a recipe book with 1,000 cookie recipes, but you don’t want to try all 1,000 recipes at once because that would take too much time and effort. Instead, you decide to bake a small group of cookies at a time—let’s say 10 recipes in one go. This group of 10 recipes is like the “batch size” in training a neural network.</p>
<p>In neural networks, the batch size refers to the number of data samples (or examples) that the model looks at before updating itself. If the batch size is small, the model updates more frequently but with less information at each step. If the batch size is large, the model updates less often but with more comprehensive information each time. It’s a trade-off between speed and accuracy of learning.</p>
</section>
<section id="epochs">
<h4><strong>Epochs</strong><a class="headerlink" href="#epochs" title="Link to this heading">#</a></h4>
<p>Now, let’s go back to our cookie analogy. Once you’ve baked all 1,000 cookie recipes by working in batches of 10, you might realize that your cookies aren’t perfect yet. So, you decide to go through all 1,000 recipes again to improve your baking skills. Each complete pass through all the recipes is called an “epoch.”</p>
<p>In neural networks, an epoch is one full cycle through the entire training dataset. If you train for multiple epochs, it means you’re giving the model multiple chances to learn from the same data. Think of it as practicing over and over again until you get better.</p>
</section>
<section id="learning-rate">
<h4><strong>Learning Rate</strong><a class="headerlink" href="#learning-rate" title="Link to this heading">#</a></h4>
<p>Imagine you’re trying to walk toward a goal on a path. If your steps are too small, it will take forever to reach your destination. If your steps are too big, you might overshoot or even trip and fall off the path. The “learning rate” in neural networks is like the size of these steps.</p>
<p>The learning rate controls how much the model adjusts itself after looking at each batch of data. A high learning rate means faster progress but with a risk of missing finer details or overshooting the goal. A low learning rate means slower progress but with more precise adjustments.</p>
</section>
<section id="loss-functions">
<h4><strong>Loss Functions</strong><a class="headerlink" href="#loss-functions" title="Link to this heading">#</a></h4>
<p>Let’s say you’ve baked a batch of cookies and taste-tested them to see how close they are to perfection based on your ideal recipe. If they’re too salty or not sweet enough, you’ll know how far off you are from your goal. This “distance” from perfection is like what we call a “loss” in neural networks.</p>
<p>The loss function measures how well (or poorly) the neural network is performing by comparing its predictions to the actual correct answers (like comparing your cookies to the ideal recipe). The goal of training is to minimize this loss so that the model gets better at making accurate predictions—just like tweaking your cookie recipe over time to make it perfect!</p>
</section>
</section>
<section id="training-process">
<h3>Training Process<a class="headerlink" href="#training-process" title="Link to this heading">#</a></h3>
<p>Let’s break down the process of training a neural network into simple steps. Think of this process like teaching a child how to ride a bicycle. Each part of the training has a specific purpose, just like the steps you’d take to help the child learn.</p>
<section id="data-preparation">
<h4>- Data Preparation<a class="headerlink" href="#data-preparation" title="Link to this heading">#</a></h4>
<p>Imagine you’re teaching the child to ride a bike in a park. Before starting, you need to choose a safe and open area where they can practice without too many obstacles. Similarly, in neural networks, we prepare and organize the data before training.</p>
<p>Data preparation involves:</p>
<ul class="simple">
<li><p><strong>Collecting data</strong>: This is like gathering information about where to practice. For example, if we are training a network to recognize cats in pictures, we need many images of cats and non-cats.</p></li>
<li><p><strong>Cleaning data</strong>: Just like clearing rocks or debris from the path to make it safe for the child, we clean our data by removing errors or irrelevant parts.</p></li>
<li><p><strong>Splitting data</strong>: We divide our data into two main parts:</p>
<ul>
<li><p><strong>Training data</strong>: This is like giving the child lots of time to practice riding.</p></li>
<li><p><strong>Validation data</strong>: This is like testing how well they can ride after practicing.</p></li>
</ul>
</li>
</ul>
<p>In short, data preparation ensures that the neural network has the right “environment” to learn effectively.</p>
</section>
<section id="model-initialization">
<h4>- Model Initialization<a class="headerlink" href="#model-initialization" title="Link to this heading">#</a></h4>
<p>Now that we have the park ready, it’s time to set up the bike. Model initialization is like adjusting the bike for the child—making sure the seat height is right and that they have training wheels if needed.</p>
<p>In neural networks, model initialization means setting up the network’s structure and starting points:</p>
<ul class="simple">
<li><p>We decide how many “layers” (like levels of difficulty) and “neurons” (like gears on a bike) are needed.</p></li>
<li><p>We also give it some initial settings (weights and biases), which are like deciding whether to start with training wheels or without them.</p></li>
</ul>
<p>This setup is important because it determines how well the network can start learning.</p>
</section>
<section id="training-loop">
<h4>- Training Loop<a class="headerlink" href="#training-loop" title="Link to this heading">#</a></h4>
<p>Now comes the actual practice—teaching the child to pedal, balance, and steer. This is where most of the learning happens. The training loop in neural networks works similarly:</p>
<ol class="arabic simple">
<li><p><strong>Input Data</strong>: The child starts pedaling (we feed input data, like images or text, into the network).</p></li>
<li><p><strong>Prediction</strong>: The child tries to move forward but may wobble at first (the network makes predictions based on its current knowledge).</p></li>
<li><p><strong>Error Calculation</strong>: If they fall or lose balance, we figure out what went wrong (the network calculates how far off its prediction was from reality).</p></li>
<li><p><strong>Adjustment</strong>: We help them adjust by holding their shoulders or giving tips (the network updates its weights and biases to improve).</p></li>
</ol>
<p>This process repeats over and over—just like practicing riding again and again until they get better.</p>
</section>
<section id="validation-steps">
<h4>- Validation Steps<a class="headerlink" href="#validation-steps" title="Link to this heading">#</a></h4>
<p>Finally, after some practice sessions, we let go of the bike for a moment to see if they can ride on their own. This is similar to validation in neural networks.</p>
<p>During validation:</p>
<ul class="simple">
<li><p>We test how well the network performs on new data it hasn’t seen before (like letting the child try riding in a different part of the park).</p></li>
<li><p>If it struggles too much, we might go back and adjust some things during training (like adding more practice time).</p></li>
</ul>
<p>Validation helps us understand whether our neural network has truly “learned” or if it’s just memorizing specific examples (like knowing only one straight path but not being able to turn).</p>
<hr class="docutils" />
<p>By following these steps—data preparation, model initialization, training loop, and validation—we teach a neural network how to solve problems step by step, just as you’d teach someone to ride a bike!</p>
</section>
</section>
</section>
<section id="common-challenges">
<h2>Common Challenges<a class="headerlink" href="#common-challenges" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<section id="overfitting">
<h3><strong>Overfitting</strong><a class="headerlink" href="#overfitting" title="Link to this heading">#</a></h3>
<p>Imagine you are teaching a child how to recognize animals, like cats and dogs. If you only show the child pictures of one specific dog (say, a golden retriever) and one specific cat (a black cat), the child might learn to recognize <em>only</em> those exact animals. When you later show them a different breed of dog, like a poodle, or a white cat, they might not recognize them at all.</p>
<p>This is what happens in overfitting. A neural network learns the training data <em>too well</em>, almost like memorizing it, instead of understanding the general idea or patterns. In real life, this means the model performs very well on the data it was trained on but struggles when faced with new data. It’s like studying only one chapter of a book for an exam and failing because the test covers different material.</p>
<hr class="docutils" />
<section id="underfitting">
<h4><strong>Underfitting</strong><a class="headerlink" href="#underfitting" title="Link to this heading">#</a></h4>
<p>Now imagine you’re teaching that same child about animals again, but this time you only give them vague descriptions like “dogs have four legs” or “cats are small.” The child might not learn enough to correctly identify animals because the information is too broad or incomplete. They might even confuse a chair with a dog because chairs also have four legs!</p>
<p>Underfitting happens when a neural network doesn’t learn enough from the training data. This could be because the model is too simple or because it wasn’t trained long enough. In real-world terms, it’s like trying to use a blurry pair of glasses—you don’t see enough detail to make accurate decisions.</p>
</section>
<hr class="docutils" />
<section id="convergence-issues">
<h4><strong>Convergence Issues</strong><a class="headerlink" href="#convergence-issues" title="Link to this heading">#</a></h4>
<p>Think about climbing a mountain blindfolded. You’re trying to reach the top (the best solution), but since you can’t see, you rely on feeling your way around. Sometimes, you might take steps that lead you downhill instead of up, or you might get stuck on a small hill thinking it’s the peak when there’s actually a taller mountain nearby.</p>
<p>In neural networks, convergence issues happen when the model struggles to find the best solution during training. It might get stuck in a “local minimum” (a small hill) instead of reaching the “global minimum” (the tallest mountain). This can happen if the learning process is poorly set up—like using steps that are too big or too small.</p>
</section>
<hr class="docutils" />
<section id="memory-management">
<h4><strong>Memory Management</strong><a class="headerlink" href="#memory-management" title="Link to this heading">#</a></h4>
<p>Imagine you’re trying to solve a jigsaw puzzle on a very small table. If your table is too tiny, you can’t lay out all the pieces at once; you’ll constantly need to shuffle pieces on and off the table to make space. This slows down your progress and makes solving the puzzle harder.</p>
<p>In neural networks, memory management works similarly. Training large models requires significant computer memory (RAM or GPU memory). If your system doesn’t have enough memory, it has to constantly move data in and out of storage, which slows down training and can even cause crashes. Efficient memory management ensures that all pieces of data fit and are processed smoothly without interruptions.</p>
</section>
</section>
</section>
<section id="optimization-techniques">
<h2>Optimization Techniques<a class="headerlink" href="#optimization-techniques" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<section id="basic-optimizers">
<h3>Basic Optimizers<a class="headerlink" href="#basic-optimizers" title="Link to this heading">#</a></h3>
<p>Optimization techniques are like tools we use to help a neural network learn. Imagine teaching a child how to ride a bike. The child starts wobbly and falls, but with practice, they adjust their balance and improve. Similarly, optimization techniques guide a neural network to adjust its parameters (or “weights”) so it can perform better at its task, like recognizing images or predicting numbers. Let’s explore the basic optimizers in simple terms.</p>
<hr class="docutils" />
<section id="gradient-descent">
<h4>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h4>
<p>Gradient Descent is like hiking down a mountain to reach the lowest point in the valley. Imagine you’re blindfolded and trying to find the bottom of the valley. You feel the slope of the ground beneath your feet and take small steps downhill, always moving in the direction that feels steepest. Each step brings you closer to the valley floor.</p>
<p>In neural networks, this “valley” represents the best possible performance (lowest error), and the “steps” are adjustments made to the network’s parameters. The slope tells us how much error there is and in which direction we should adjust to reduce it.</p>
</section>
<hr class="docutils" />
<section id="stochastic-gradient-descent-sgd">
<h4>Stochastic Gradient Descent (SGD)<a class="headerlink" href="#stochastic-gradient-descent-sgd" title="Link to this heading">#</a></h4>
<p>Now, imagine instead of walking down the mountain in a calm, steady way, you’re in a storm where gusts of wind push you around unpredictably as you descend. This is what Stochastic Gradient Descent feels like.</p>
<p>In SGD, instead of calculating the slope using all the data at once (like in regular Gradient Descent), we use just one random piece of data at a time. This makes the process faster but also noisier—sometimes it feels like you’re taking steps in the wrong direction because of those random gusts (errors). However, over time, you still manage to reach the valley floor.</p>
</section>
<hr class="docutils" />
<section id="mini-batch-gradient-descent">
<h4>Mini-batch Gradient Descent<a class="headerlink" href="#mini-batch-gradient-descent" title="Link to this heading">#</a></h4>
<p>Mini-batch Gradient Descent is like hiking down the mountain with a group of friends. Instead of relying on just one person’s sense of direction (like in SGD) or waiting for everyone to agree on the best path (like in full Gradient Descent), you form smaller groups to decide where to step next.</p>
<p>Here, instead of using all the data or just one piece, we split the data into small groups called “mini-batches.” Each mini-batch gives us an idea of which direction to go, balancing speed and accuracy. It’s a middle ground between regular Gradient Descent and Stochastic Gradient Descent.</p>
</section>
<hr class="docutils" />
<section id="learning-rate-schedules">
<h4>Learning Rate Schedules<a class="headerlink" href="#learning-rate-schedules" title="Link to this heading">#</a></h4>
<p>The learning rate is like deciding how big your steps should be while hiking down that mountain. If your steps are too big, you might overshoot and miss the valley floor entirely. If they’re too small, it’ll take forever to get there.</p>
<p>A Learning Rate Schedule adjusts your step size as you go along. For example:</p>
<ul class="simple">
<li><p>At first, you might take big steps because you’re far from the valley floor and need to cover more ground quickly.</p></li>
<li><p>As you get closer to the bottom, your steps become smaller and more careful so you don’t overshoot or wobble around too much.</p></li>
</ul>
<p>This ensures that your learning process is both efficient and precise.</p>
<hr class="docutils" />
<p>By understanding these optimizers and their real-world analogies, we can see how neural networks gradually improve their performance through trial and error—just like learning any new skill!</p>
</section>
</section>
<section id="advanced-optimizers">
<h3>Advanced Optimizers<a class="headerlink" href="#advanced-optimizers" title="Link to this heading">#</a></h3>
<p>Optimization techniques are methods used to adjust the parameters (weights and biases) of a neural network so that it performs better at its task, like recognizing images or predicting numbers. Think of it like trying to find the quickest and smoothest path to the top of a hill (or the bottom of a valley, in this case). Each optimizer has its own way of figuring out how to move closer to the goal. Below, we’ll explain some advanced optimizers using simple analogies.</p>
<hr class="docutils" />
<section id="adam">
<h4><strong>Adam</strong><a class="headerlink" href="#adam" title="Link to this heading">#</a></h4>
<p>Adam stands for <em>Adaptive Moment Estimation</em>. Imagine you are hiking in a hilly area, trying to find the lowest point in a valley (this represents minimizing error in your neural network). However, the terrain is uneven, and sometimes you might take a step that’s too big or too small. Adam helps by combining two smart strategies:</p>
<ol class="arabic simple">
<li><p><strong>Momentum</strong>: It keeps track of the direction you’ve been moving in and gives you a little push in that direction, like rolling down a hill with some speed.</p></li>
<li><p><strong>Adaptiveness</strong>: It adjusts how big your steps are based on how steep or flat the terrain is. If you’re on a steep slope, it takes smaller steps to avoid overshooting. If the slope is gentle, it takes bigger steps to speed up progress.</p></li>
</ol>
<p>In real life, Adam is like using both a map and a compass while hiking—one helps you know where you are heading (momentum), and the other adjusts your pace depending on the difficulty of the trail.</p>
</section>
<hr class="docutils" />
<section id="rmsprop">
<h4><strong>RMSprop</strong><a class="headerlink" href="#rmsprop" title="Link to this heading">#</a></h4>
<p>RMSprop stands for <em>Root Mean Square Propagation</em>. Let’s go back to our hiking analogy. Imagine that as you hike, you notice that some parts of the trail are rocky and uneven (steep gradients), while others are smooth and easy to walk on (flat gradients). RMSprop is like wearing shoes with special soles that adapt to these conditions. In rocky areas, they help you take smaller, more careful steps so you don’t trip. On smooth trails, they let you take bigger strides.</p>
<p>RMSprop works by dividing your step size by how much variation (bumpiness) there is in the terrain. This ensures that no matter how rough or smooth the path gets, you can adjust your steps accordingly.</p>
</section>
<hr class="docutils" />
<section id="adagrad">
<h4><strong>AdaGrad</strong><a class="headerlink" href="#adagrad" title="Link to this heading">#</a></h4>
<p>AdaGrad stands for <em>Adaptive Gradient Algorithm</em>. Imagine you’re learning how to ride a bike on different types of roads—some are bumpy gravel paths, while others are smooth asphalt. At first, you’re cautious and take small steps everywhere because you don’t know what’s coming. Over time, AdaGrad remembers which parts of the road were bumpiest and adjusts your riding style.</p>
<p>In simpler terms, AdaGrad gives smaller step sizes for areas where there’s been a lot of learning already (like focusing less on paths you’ve already mastered) and larger step sizes for unexplored areas (like paying more attention to tricky new paths). This makes it great for problems where some parts of the data need more focus than others.</p>
</section>
<hr class="docutils" />
<section id="momentum">
<h4><strong>Momentum</strong><a class="headerlink" href="#momentum" title="Link to this heading">#</a></h4>
<p>Momentum is one of the simplest ideas but very powerful. Imagine pushing a heavy shopping cart down an aisle in a store. At first, it takes effort to get it moving because it’s heavy (this is like starting optimization when your neural network doesn’t know much). But once it starts rolling, it becomes easier to keep it moving because momentum builds up.</p>
<p>In optimization terms, momentum remembers the direction you’ve been moving in and keeps pushing you forward even if there’s a slight uphill or resistance. This helps avoid getting stuck in small dips or bumps along the way (local minima) and makes sure progress doesn’t stop prematurely.</p>
<hr class="docutils" />
<p>These advanced optimizers combine clever strategies to make training neural networks faster and more efficient. Each one has its strengths depending on how “bumpy” or “smooth” your problem’s landscape is!</p>
</section>
</section>
</section>
<section id="id7">
<h2>Optimization Techniques<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<section id="fine-tuning">
<h3>Fine-tuning<a class="headerlink" href="#fine-tuning" title="Link to this heading">#</a></h3>
<p>Fine-tuning in neural networks is like adjusting the settings of a machine to make it work more efficiently. Imagine you are tuning a guitar. Initially, the strings might be too tight or too loose, and the sound is off. By carefully adjusting each string, you can make the guitar produce beautiful music. Similarly, in neural networks, fine-tuning involves tweaking various aspects of the model to make it perform better on a task.</p>
<section id="hyperparameter-optimization">
<h4>Hyperparameter Optimization<a class="headerlink" href="#hyperparameter-optimization" title="Link to this heading">#</a></h4>
<p>Think of hyperparameters as the settings or knobs on a washing machine. When you’re doing laundry, you adjust settings like water temperature, spin speed, or cycle type depending on the clothes you’re washing. If you choose the wrong settings, your clothes might not get cleaned properly or could even get damaged.</p>
<p>In neural networks, hyperparameters include things like the learning rate (how fast the model learns), batch size (how much data it processes at once), or the number of layers in the network. Hyperparameter optimization is about finding the best combination of these settings to ensure your model “cleans” (learns) efficiently without overloading or underperforming.</p>
</section>
<section id="early-stopping">
<h4>Early Stopping<a class="headerlink" href="#early-stopping" title="Link to this heading">#</a></h4>
<p>Imagine you’re baking cookies. You set a timer for 10 minutes, but you keep an eye on them through the oven door. If they start to look golden brown at 8 minutes, you take them out early to avoid burning them. Early stopping in neural networks works similarly.</p>
<p>As a model learns from data, its performance on training data improves steadily. However, if it trains for too long, it might start “overfitting,” which means it memorizes the training data too much and performs poorly on new data. Early stopping monitors performance and halts training when the model reaches its best point before overfitting starts—just like taking cookies out of the oven at the perfect time.</p>
</section>
<section id="regularization">
<h4>Regularization<a class="headerlink" href="#regularization" title="Link to this heading">#</a></h4>
<p>Regularization is like packing for a trip with limited luggage space. If you try to pack everything you own, your suitcase becomes cluttered and hard to manage. Instead, you prioritize only what’s necessary for your trip and leave out unnecessary items.</p>
<p>In neural networks, regularization helps prevent overfitting by simplifying the model. It discourages the model from focusing too much on specific details in training data that might not generalize well to new data. This keeps the model efficient and focused on what really matters.</p>
</section>
<section id="dropout">
<h4>Dropout<a class="headerlink" href="#dropout" title="Link to this heading">#</a></h4>
<p>Dropout is like randomly turning off some lights in a house to save energy. Imagine you’re in a large house with many rooms lit up, but not all lights are needed at once. By turning off some lights randomly, you save energy without compromising your ability to see where you’re going.</p>
<p>In neural networks, dropout randomly “turns off” some neurons during training so that they don’t participate in learning for that round. This prevents any single neuron from becoming too dominant or overly reliant on others, making the network more robust and better at generalizing to new data.</p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Chapter%204%20-%20Unsupervised%20Learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 4 - Unsupervised Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="Chapter%206%20-%20Deep%20Learning%20Tools.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 6 - Deep Learning Tools</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Chapter 5 - Neural Networks Basics</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-neural-networks">Artificial Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#structure-basics">Structure Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neurons-and-connections">Neurons and Connections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layers-explained">Layers Explained</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input-layer">Input Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-layers">Hidden Layers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output-layer">Output Layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-architecture">Network Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feed-forward-networks">Feed-Forward Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#network-topology">Network Topology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-connections">Layer Connections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-and-bias">Weight and Bias</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-neural-networks-anns"><strong>Artificial Neural Networks (ANNs)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-neural-networks"><strong>Types of Neural Networks</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-layer-neural-network"><strong>1. Single-Layer Neural Network</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#real-life-analogy">🔍 <strong>Real-life analogy:</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-layer-neural-network"><strong>2. Multi-Layer Neural Network</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">🔍 <strong>Real-life analogy:</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-neural-networks-dnns"><strong>3. Deep Neural Networks (DNNs)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">🔍 <strong>Real-life analogy:</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-architectures"><strong>4. Common Architectures</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-feedforward-neural-network-fnn"><strong>a. Feedforward Neural Network (FNN)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-convolutional-neural-network-cnn"><strong>b. Convolutional Neural Network (CNN)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#c-recurrent-neural-network-rnn"><strong>c. Recurrent Neural Network (RNN)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#d-long-short-term-memory-lstm"><strong>d. Long Short-Term Memory (LSTM)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#e-generative-adversarial-network-gan"><strong>e. Generative Adversarial Network (GAN)</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary"><strong>Summary</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-functions">Common Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function"><strong>Sigmoid Function</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#relu-rectified-linear-unit"><strong>ReLU (Rectified Linear Unit)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh-hyperbolic-tangent-function"><strong>Tanh (Hyperbolic Tangent Function)</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#leaky-relu"><strong>Leaky ReLU</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Chapter 5: Neural Networks Basics</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selection-criteria"><strong>Selection Criteria</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-disadvantages"><strong>Advantages/Disadvantages</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vanishing-gradient-problem"><strong>Vanishing Gradient Problem</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modern-solutions"><strong>Modern Solutions</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Activation Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation"><strong>Implementation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#input-processing"><strong>1. Input Processing</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output-range"><strong>2. Output Range</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-efficiency"><strong>3. Computational Efficiency</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#best-practices"><strong>4. Best Practices</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation"><strong>Forward Propagation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><strong>1. Input Processing</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-calculations"><strong>2. Layer Calculations</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#signal-flow"><strong>3. Signal Flow</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#output-generation"><strong>4. Output Generation</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-propagation">Backward Propagation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#error-calculation"><strong>Error Calculation</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule"><strong>Chain Rule</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-computation"><strong>Gradient Computation</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-updates"><strong>Weight Updates</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-process"><strong>Learning Process</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#information-flow"><strong>1. Information Flow</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#error-distribution"><strong>2. Error Distribution</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-adjustment"><strong>3. Weight Adjustment</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-learning"><strong>4. Iterative Learning</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-neural-networks">Training Neural Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-training-concepts">Basic Training Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-size"><strong>Batch Size</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#epochs"><strong>Epochs</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate"><strong>Learning Rate</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions"><strong>Loss Functions</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process">Training Process</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">- Data Preparation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#model-initialization">- Model Initialization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop">- Training Loop</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-steps">- Validation Steps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-challenges">Common Challenges</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting"><strong>Overfitting</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#underfitting"><strong>Underfitting</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence-issues"><strong>Convergence Issues</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-management"><strong>Memory Management</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-techniques">Optimization Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-optimizers">Basic Optimizers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-batch-gradient-descent">Mini-batch Gradient Descent</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-schedules">Learning Rate Schedules</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-optimizers">Advanced Optimizers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#adam"><strong>Adam</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rmsprop"><strong>RMSprop</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#adagrad"><strong>AdaGrad</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum"><strong>Momentum</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Optimization Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-optimization">Hyperparameter Optimization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">Early Stopping</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">Regularization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Shreyash Gupta
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>