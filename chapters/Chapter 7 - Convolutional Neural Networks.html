
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 7 - Convolutional Neural Networks &#8212; Machine Learning for Dummies</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=f0c89327" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=d2032c04" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=f0c89327" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/theme.css?v=a243ae73" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/Chapter 7 - Convolutional Neural Networks';</script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/searchtools.js?v=63a53a7d"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/language_data.js?v=d4673a71"></script>
    <script src="../_static/copybutton_funcs.js?v=776a791e"></script>
    <script src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/scripts/bootstrap.js?v=7583a70d"></script>
    <script src="../_static/scripts/fontawesome.js?v=9b125980"></script>
    <script src="../_static/scripts/pydata-sphinx-theme.js?v=f62441ba"></script>
    <link rel="icon" href="../_static/course-logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 8 - Sequential Data and RNNs" href="Chapter%208%20-%20Sequential%20Data%20and%20RNNs.html" />
    <link rel="prev" title="Chapter 6 - Deep Learning Tools" href="Chapter%206%20-%20Deep%20Learning%20Tools.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/course-logo.png" class="logo__image only-light" alt="Machine Learning for Dummies - Home"/>
    <img src="../_static/course-logo.png" class="logo__image only-dark pst-js-only" alt="Machine Learning for Dummies - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundations of Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Chapter%201%20-%20Introduction%20to%20Machine%20Learning.html">Chapter 1: Introduction to Machine Learning</a></li>

<li class="toctree-l1"><a class="reference internal" href="Chapter%202%20-%20Data%20Fundamentals.html">Chapter 2 - Data Fundamentals</a></li>







<li class="toctree-l1"><a class="reference internal" href="Chapter%203%20-%20Supervised%20Learning.html">Chapter 3 - Supervised Learning</a></li>







<li class="toctree-l1"><a class="reference internal" href="Chapter%204%20-%20Unsupervised%20Learning.html">Chapter 4 - Unsupervised Learning</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Fundamentals</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Chapter%205%20-%20Neural%20Networks%20Basics.html">Chapter 5 - Neural Networks Basics</a></li>

<li class="toctree-l1"><a class="reference internal" href="Chapter%206%20-%20Deep%20Learning%20Tools.html">Chapter 6 - Deep Learning Tools</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 7 - Convolutional Neural Networks</a></li>

<li class="toctree-l1"><a class="reference internal" href="Chapter%208%20-%20Sequential%20Data%20and%20RNNs.html">Chapter 8 - Sequential Data and RNNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chapter%209%20-%20Modern%20Deep%20Learning.html">Chapter 9 - Modern Deep Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/shreyashguptas/Machine-Learning-for-Dummies/blob/main/chapters/Chapter 7 - Convolutional Neural Networks.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/shreyashguptas/Machine-Learning-for-Dummies" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/shreyashguptas/Machine-Learning-for-Dummies/edit/main/chapters/Chapter 7 - Convolutional Neural Networks.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/shreyashguptas/Machine-Learning-for-Dummies/issues/new?title=Issue%20on%20page%20%2Fchapters/Chapter 7 - Convolutional Neural Networks.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/Chapter 7 - Convolutional Neural Networks.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 7 - Convolutional Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Chapter 7 - Convolutional Neural Networks</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Chapter 7: Convolutional Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-processing-basics">Image Processing Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-digital-images">Understanding Digital Images</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-properties">Image Properties</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn-architecture">CNN Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-components">Basic Components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-organization">Layer Organization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-concepts">Advanced Concepts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-and-padding">Pooling and Padding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-methods">Pooling Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#padding-techniques">Padding Techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality">Dimensionality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">Transfer Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-trained-models">Pre-trained Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#best-practices">Best Practices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computer-vision-applications">Computer Vision Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-classification">Image Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#object-detection">Object Detection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-applications">Advanced Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Image Processing Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Understanding Digital Images</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pixels-and-colors">Pixels and Colors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rgb-channels">RGB Channels</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#image-resolution">Image Resolution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#color-depth">Color Depth</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Image Processing Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Image Properties</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-detection">Feature Detection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#edge-recognition">Edge Recognition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pattern-identification">Pattern Identification</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#image-preprocessing">Image Preprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">CNN Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Basic Components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Layer Organization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Advanced Concepts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Pooling and Padding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Pooling Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Padding Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#valid-padding">Valid Padding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#same-padding">Same Padding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-padding">Types of Padding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#impact-on-output">Impact on Output</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Pooling and Padding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Dimensionality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Transfer Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Pre-trained Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Best Practices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Computer Vision Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Image Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Object Detection</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bounding-boxes">Bounding Boxes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#object-localization">Object Localization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-objects">Multiple Objects</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#detection-networks">Detection Networks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#face-recognition">Face Recognition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-segmentation">Image Segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#style-transfer">Style Transfer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#video-processing">Video Processing</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-7-convolutional-neural-networks">
<h1>Chapter 7 - Convolutional Neural Networks<a class="headerlink" href="#chapter-7-convolutional-neural-networks" title="Link to this heading">#</a></h1>
<hr class="docutils" />
<p>Let me break down Chapter 7 into beginner-friendly subtopics:</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>Chapter 7: Convolutional Neural Networks<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<section id="image-processing-basics">
<h2>Image Processing Basics<a class="headerlink" href="#image-processing-basics" title="Link to this heading">#</a></h2>
<section id="understanding-digital-images">
<h3>Understanding Digital Images<a class="headerlink" href="#understanding-digital-images" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Pixels and Colors</p></li>
<li><p>RGB Channels</p></li>
<li><p>Image Resolution</p></li>
<li><p>Color Depth</p></li>
</ul>
</section>
<section id="image-properties">
<h3>Image Properties<a class="headerlink" href="#image-properties" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature Detection</p></li>
<li><p>Edge Recognition</p></li>
<li><p>Pattern Identification</p></li>
<li><p>Image Preprocessing</p></li>
</ul>
</section>
</section>
<section id="cnn-architecture">
<h2>CNN Architecture<a class="headerlink" href="#cnn-architecture" title="Link to this heading">#</a></h2>
<section id="basic-components">
<h3>Basic Components<a class="headerlink" href="#basic-components" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Convolutional Layers</p></li>
<li><p>Filters/Kernels</p></li>
<li><p>Feature Maps</p></li>
<li><p>Stride and Dilation</p></li>
</ul>
</section>
<section id="layer-organization">
<h3>Layer Organization<a class="headerlink" href="#layer-organization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Input Layer</p></li>
<li><p>Hidden Layers</p></li>
<li><p>Fully Connected Layers</p></li>
<li><p>Output Layer</p></li>
</ul>
</section>
<section id="advanced-concepts">
<h3>Advanced Concepts<a class="headerlink" href="#advanced-concepts" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Receptive Field</p></li>
<li><p>Channel Depth</p></li>
<li><p>Parameter Sharing</p></li>
<li><p>Local Connectivity</p></li>
</ul>
</section>
</section>
<section id="pooling-and-padding">
<h2>Pooling and Padding<a class="headerlink" href="#pooling-and-padding" title="Link to this heading">#</a></h2>
<section id="pooling-methods">
<h3>Pooling Methods<a class="headerlink" href="#pooling-methods" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Max Pooling</p></li>
<li><p>Average Pooling</p></li>
<li><p>Global Pooling</p></li>
<li><p>When to Use Each</p></li>
</ul>
</section>
<section id="padding-techniques">
<h3>Padding Techniques<a class="headerlink" href="#padding-techniques" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Valid Padding</p></li>
<li><p>Same Padding</p></li>
<li><p>Types of Padding</p></li>
<li><p>Impact on Output</p></li>
</ul>
</section>
<section id="dimensionality">
<h3>Dimensionality<a class="headerlink" href="#dimensionality" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Input Size Changes</p></li>
<li><p>Feature Map Size</p></li>
<li><p>Output Dimensions</p></li>
<li><p>Size Calculations</p></li>
</ul>
</section>
</section>
<section id="transfer-learning">
<h2>Transfer Learning<a class="headerlink" href="#transfer-learning" title="Link to this heading">#</a></h2>
<section id="pre-trained-models">
<h3>Pre-trained Models<a class="headerlink" href="#pre-trained-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Popular Architectures</p></li>
<li><p>Model Zoo</p></li>
<li><p>Weight Transfer</p></li>
<li><p>Fine-tuning</p></li>
</ul>
</section>
<section id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature Extraction</p></li>
<li><p>Layer Freezing</p></li>
<li><p>Model Adaptation</p></li>
<li><p>Training Strategies</p></li>
</ul>
</section>
<section id="best-practices">
<h3>Best Practices<a class="headerlink" href="#best-practices" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>When to Use</p></li>
<li><p>Model Selection</p></li>
<li><p>Performance Tips</p></li>
<li><p>Common Pitfalls</p></li>
</ul>
</section>
</section>
<section id="computer-vision-applications">
<h2>Computer Vision Applications<a class="headerlink" href="#computer-vision-applications" title="Link to this heading">#</a></h2>
<section id="image-classification">
<h3>Image Classification<a class="headerlink" href="#image-classification" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Single Label</p></li>
<li><p>Multi-Label</p></li>
<li><p>Hierarchical Classification</p></li>
<li><p>Real-world Examples</p></li>
</ul>
</section>
<section id="object-detection">
<h3>Object Detection<a class="headerlink" href="#object-detection" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Bounding Boxes</p></li>
<li><p>Object Localization</p></li>
<li><p>Multiple Objects</p></li>
<li><p>Detection Networks</p></li>
</ul>
</section>
<section id="advanced-applications">
<h3>Advanced Applications<a class="headerlink" href="#advanced-applications" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Face Recognition</p></li>
<li><p>Image Segmentation</p></li>
<li><p>Style Transfer</p></li>
<li><p>Video Processing</p></li>
</ul>
<p>Remember for each topic:</p>
<ul class="simple">
<li><p>Start with simple analogies</p></li>
<li><p>Use real-world examples</p></li>
<li><p>Include visual explanations</p></li>
<li><p>Keep mathematical complexity low</p></li>
<li><p>Focus on practical understanding</p></li>
</ul>
<p>Think of this chapter like teaching someone to:</p>
<ul class="simple">
<li><p>Understand images (Image Processing)</p></li>
<li><p>Build image recognition systems (CNN)</p></li>
<li><p>Handle different image sizes (Pooling/Padding)</p></li>
<li><p>Use existing knowledge (Transfer Learning)</p></li>
<li><p>Solve real problems (Applications)</p></li>
</ul>
<p>Each section should build upon the previous one, creating a natural progression from basic concepts to practical applications in computer vision.</p>
</section>
</section>
<section id="id2">
<h2>Image Processing Basics<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<section id="id3">
<h3>Understanding Digital Images<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<section id="pixels-and-colors">
<h4>Pixels and Colors<a class="headerlink" href="#pixels-and-colors" title="Link to this heading">#</a></h4>
<p>Imagine a digital image as a large mosaic made up of tiny tiles, where each tile is called a <strong>pixel</strong>. Just like in a mosaic, each pixel in an image has a specific color. The color of each pixel is determined by combining different amounts of three primary colors: red, green, and blue. These are known as the <strong>RGB channels</strong>. By adjusting the intensity of these colors, you can create any color you see in a digital image. Think of it like mixing paint; by varying how much red, green, and blue you mix, you can produce different shades and colors.</p>
</section>
<section id="rgb-channels">
<h4>RGB Channels<a class="headerlink" href="#rgb-channels" title="Link to this heading">#</a></h4>
<p>In digital images, each pixel’s color is often represented using three numbers that correspond to the intensity of red, green, and blue light. For instance, if you imagine a flashlight with red, green, and blue bulbs, you can create different colors by turning these bulbs on or off at varying intensities. If all three are on at full intensity, you get white light; if they are all off, you get black. This is how RGB channels work in images: they combine to create the full spectrum of colors we see.</p>
</section>
<section id="image-resolution">
<h4>Image Resolution<a class="headerlink" href="#image-resolution" title="Link to this heading">#</a></h4>
<p>Resolution refers to the number of pixels in an image. It’s like comparing two mosaics: one with more tiles will have more detail than one with fewer tiles. A higher resolution means more pixels are used to display the image, resulting in finer detail and clarity. In practical terms, when you zoom into a high-resolution image, it stays clear longer than a low-resolution one before becoming blurry.</p>
</section>
<section id="color-depth">
<h4>Color Depth<a class="headerlink" href="#color-depth" title="Link to this heading">#</a></h4>
<p>Color depth is akin to the variety of colors available in your paint set. The greater the color depth, the more colors you can use to paint your picture. In digital terms, color depth refers to how many bits are used for each color channel (red, green, and blue). More bits mean more possible color combinations and thus richer and more detailed images. For example, an 8-bit per channel system can display 256 shades per channel (red, green, or blue), leading to over 16 million possible colors when combined.</p>
<p>These concepts form the foundation of understanding how digital images are constructed and manipulated in various applications like photography, video production, and computer graphics.</p>
</section>
</section>
</section>
<section id="id4">
<h2>Image Processing Basics<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<section id="id5">
<h3>Image Properties<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>Understanding the basics of image properties is crucial for working with Convolutional Neural Networks (CNNs). Let’s break this down into simple concepts using real-life analogies.</p>
<hr class="docutils" />
<section id="feature-detection">
<h4>Feature Detection<a class="headerlink" href="#feature-detection" title="Link to this heading">#</a></h4>
<p>Imagine you are looking at a picture of a house. The first thing you might notice are the basic elements, like the shape of the roof, the windows, and the door. These are <strong>features</strong>—distinct parts of an image that help us recognize what we are looking at. In a CNN, feature detection is like teaching a computer to notice these parts.</p>
<p>For instance:</p>
<ul class="simple">
<li><p>A <strong>filter</strong> in a CNN acts like a stencil that slides over an image to highlight specific features like edges or textures.</p></li>
<li><p>Think of it as using a magnifying glass to focus on certain details of a painting.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="edge-recognition">
<h4>Edge Recognition<a class="headerlink" href="#edge-recognition" title="Link to this heading">#</a></h4>
<p>Edges in an image are where there is a sharp change in color or brightness, like the outline of an object. Imagine tracing the outline of a leaf with your finger. You’re essentially identifying its edges.</p>
<p>In real life:</p>
<ul class="simple">
<li><p>When you see a pencil sketch, your brain recognizes objects by their outlines or edges.</p></li>
<li><p>Similarly, in image processing, edge recognition helps identify the boundaries of objects, such as where the roof ends and the sky begins in a photo of a house.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="pattern-identification">
<h4>Pattern Identification<a class="headerlink" href="#pattern-identification" title="Link to this heading">#</a></h4>
<p>Patterns are recurring arrangements in an image, like stripes on a zebra or tiles on a floor. Identifying patterns is like recognizing that zebra stripes are always black and white and follow a certain order.</p>
<p>Think about:</p>
<ul class="simple">
<li><p>Looking at wallpaper with floral designs. Even if you only see part of it, you can guess what the rest looks like because of the repeating pattern.</p></li>
<li><p>CNNs use this concept to detect patterns in images, which helps them recognize objects regardless of their position or orientation.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="image-preprocessing">
<h4>Image Preprocessing<a class="headerlink" href="#image-preprocessing" title="Link to this heading">#</a></h4>
<p>Before analyzing an image, we often need to prepare it so it’s easier to work with—this is called preprocessing. Imagine cleaning your glasses before reading; preprocessing ensures clarity.</p>
<p>Some common steps include:</p>
<ul class="simple">
<li><p><strong>Resizing</strong>: Like cropping or zooming into a photo to fit it into a frame.</p></li>
<li><p><strong>Normalizing brightness</strong>: Adjusting lighting in an image so it’s neither too dark nor too bright, similar to adjusting your camera settings for better visibility.</p></li>
<li><p><strong>Removing noise</strong>: Think of this as erasing smudges on a photograph so you can see it clearly.</p></li>
</ul>
<hr class="docutils" />
<p>By understanding these basic properties—features, edges, patterns, and preprocessing—you’re equipping yourself with foundational knowledge to explore how CNNs analyze images!</p>
</section>
</section>
</section>
<section id="id6">
<h2>CNN Architecture<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<p>Convolutional Neural Networks (CNNs) are a type of deep learning model primarily used for analyzing visual data. They mimic the way humans perceive images, breaking them down into simpler parts and gradually building up to more complex structures. Let’s explore the basic components of CNN architecture using simple analogies.</p>
<section id="id7">
<h3>Basic Components<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p><strong>Convolutional Layers</strong></p>
<p>Imagine you’re looking at a large, detailed painting. To understand it, you might use a small magnifying glass to examine different sections one at a time. This is similar to what convolutional layers do in a CNN. They use small windows, known as filters or kernels, to scan over an image and capture essential features like edges or textures[1][2]. These filters are like the magnifying glass, focusing on small parts of the image to identify patterns.</p>
<p><strong>Filters/Kernels</strong></p>
<p>Think of filters as cookie cutters that shape dough into specific forms. In CNNs, filters are small grids that slide over the image to detect specific patterns or features, such as lines or colors[3][4]. Each filter is designed to recognize a particular feature, and as it moves across the image, it creates a new representation called a feature map.</p>
<p><strong>Feature Maps</strong></p>
<p>Once the filter has scanned the entire image, it produces a feature map. Imagine using your magnifying glass across different sections of the painting; each section reveals different details. Similarly, feature maps capture these details and help the CNN understand what features are present in different parts of the image[5][6]. These maps are then used in subsequent layers to build a more comprehensive understanding of the image.</p>
<p><strong>Stride and Dilation</strong></p>
<p>Stride refers to how far the filter moves with each step as it scans the image. If you were using your magnifying glass with large strides, you’d skip over some details, moving quickly across the painting. Smaller strides mean you move slowly and capture more detail[7]. Dilation is like stretching your magnifying glass so it can see more of the painting at once without moving. This allows the network to capture patterns that are spread out over larger areas[8].</p>
<p>By understanding these components, you can see how CNNs process images similarly to how we might examine and interpret visual information in our everyday lives. Each layer builds upon the last, gradually constructing a detailed understanding of what an image represents.</p>
</section>
<section id="id8">
<h3>Layer Organization<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<p><strong>Input Layer</strong></p>
<p>Imagine you are looking at a picture. The input layer of a Convolutional Neural Network (CNN) is like your eyes, receiving the raw image data. Just as your eyes capture the colors and shapes in a photograph, the input layer takes in all the pixel values of an image. This layer doesn’t do any processing; it simply passes on the raw data to the next layers for further analysis.</p>
<p><strong>Hidden Layers</strong></p>
<p>The hidden layers in a CNN are where the magic happens, much like how your brain processes the visual information received by your eyes. These layers include convolutional layers, pooling layers, and activation functions.</p>
<ul class="simple">
<li><p><strong>Convolutional Layers:</strong> Think of these as a series of filters or lenses that help to highlight different features of an image, such as edges or textures. Imagine you have a magnifying glass that allows you to see specific details of an object. Similarly, convolutional layers apply various filters to extract important features from the input image.</p></li>
<li><p><strong>Pooling Layers:</strong> These layers act like a zoom-out function on a camera, summarizing regions of the image to reduce its size while retaining essential information. It’s like looking at a landscape from a distance; you can’t see every single detail, but you can still understand the overall scene.</p></li>
<li><p><strong>Activation Functions:</strong> These are like decision-makers that determine which features are important enough to pass on to the next layer. They introduce non-linearity into the model, allowing it to learn complex patterns.</p></li>
</ul>
<p><strong>Fully Connected Layers</strong></p>
<p>Once the image has been processed through several hidden layers, it reaches the fully connected layers. These layers are similar to how our brain makes final decisions based on all the processed information. Imagine you’ve gathered all clues and evidence about something and now need to make a conclusion. In a CNN, fully connected layers take all the features extracted from previous layers and combine them to make predictions or classifications about the input image.</p>
<p><strong>Output Layer</strong></p>
<p>Finally, we reach the output layer, which is like delivering your verdict based on everything you’ve observed and analyzed. If you’re using a CNN for image classification, this layer will provide the final result—such as identifying whether an image is of a cat or a dog. The output layer translates all the learned features into understandable categories or labels, giving us meaningful insights from raw data.</p>
</section>
<section id="id9">
<h3>Advanced Concepts<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>Receptive Field</strong></p>
<p>Imagine you’re looking out of a window. The part of the world you can see through the window is like a <em>receptive field</em>. In the context of Convolutional Neural Networks (CNNs), a receptive field refers to the region of the input image that a particular neuron in a layer “sees.” As you move deeper into the network, neurons have larger receptive fields, meaning they can see more of the image. This is similar to how moving back from the window allows you to see more of the outside world. The receptive field is crucial because it determines how much context a neuron has when making decisions about what features are present in an image.</p>
</li>
<li><p><strong>Channel Depth</strong></p>
<p>Think of channel depth like layers of transparent colored films stacked on top of each other. Each film adds a different color or detail to the overall picture. In CNNs, channel depth refers to the number of feature maps in each layer. Each channel captures different aspects or features of the input image, such as edges or textures. Just like how combining different colored films can give you a richer image, having multiple channels allows a CNN to capture more complex features from the input data.</p>
</li>
<li><p><strong>Parameter Sharing</strong></p>
<p>Consider a rubber stamp with a pattern on it. You can use this stamp to create multiple copies of the pattern across different parts of a page without needing to recreate the pattern each time. In CNNs, parameter sharing is similar; it involves using the same set of weights (like the stamp) across different parts of an image. This means that instead of learning separate parameters for each position in an image, CNNs learn one set of parameters that can be applied across various locations. This reduces the number of parameters needed and helps the network generalize better by recognizing patterns regardless of their position in the image.</p>
</li>
<li><p><strong>Local Connectivity</strong></p>
<p>Imagine you’re piecing together a large jigsaw puzzle. You focus on connecting pieces that are close to each other rather than trying to connect pieces from opposite ends of the puzzle. Local connectivity in CNNs works similarly; neurons are connected only to a small region of the input image rather than to every pixel. This approach mimics how we process visual information locally and helps reduce computational complexity by focusing on nearby pixels, which are more likely to be related. This way, CNNs efficiently capture local patterns and details within an image before combining them at higher layers for more global understanding.</p>
</li>
</ul>
</section>
</section>
<section id="id10">
<h2>Pooling and Padding<a class="headerlink" href="#id10" title="Link to this heading">#</a></h2>
<section id="id11">
<h3>Pooling Methods<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Max Pooling</strong><br />
Imagine you have a camera that captures a very detailed picture of a forest. You want to identify the tallest tree in each small section of the forest. Instead of looking at every single tree in detail, you focus only on the tallest one in each section. This is what max pooling does—it looks at small regions of an image (like 2x2 or 3x3 grids) and keeps only the largest value (the “tallest tree”). This helps simplify the data while keeping the most important features, like edges or bright spots.</p></li>
<li><p><strong>Average Pooling</strong><br />
Now, let’s say instead of focusing just on the tallest tree, you want to know the average height of all trees in each section of the forest. This is what average pooling does—it calculates the average value of all pixels in a small region. While it also simplifies the data, it smooths out details and can sometimes lose sharpness compared to max pooling.</p></li>
<li><p><strong>Global Pooling</strong><br />
Imagine you’re looking at the entire forest and want to summarize it with just one number, like either the height of the tallest tree (global max pooling) or the average height of all trees (global average pooling). Global pooling takes all values from an image and reduces them to a single value for each feature map. This is especially useful when you want to make predictions without worrying about the size of your input image.</p></li>
<li><p><strong>When to Use Each</strong><br />
Max pooling is great when you want to focus on sharp features like edges or patterns, as it preserves strong signals. Average pooling works well when you’re more interested in general trends or smoothing out noise. Global pooling is often used at the end of a network when you need to summarize everything into a single prediction, like classifying an entire image as “cat” or “dog.”</p></li>
</ul>
</section>
<section id="id12">
<h3>Padding Techniques<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<p>Padding in convolutional neural networks is like adding a border around an image to make it easier to process. Imagine you are painting a picture on a canvas, but the edges of the canvas are so tight that your brush can’t move freely. Adding padding is like extending the canvas edges so you can paint without restrictions. In CNNs, padding ensures that the filters (small windows that slide over the image) can cover the entire image, even at the edges.</p>
<section id="valid-padding">
<h4>Valid Padding<a class="headerlink" href="#valid-padding" title="Link to this heading">#</a></h4>
<p>Valid padding is like trimming off the edges of your picture instead of extending them. When using valid padding, no extra border is added to the image. This means the filter only processes the parts of the image it can fully cover, and anything that doesn’t fit is ignored. For example, if you have a 5x5 grid and use a 3x3 filter, valid padding will shrink your output because the filter cannot fully process the edges.</p>
<p>Real-life analogy: Think of cutting out cookies with a cookie cutter from dough. If your cutter doesn’t fit at the edges of the dough, you simply don’t use those parts.</p>
</section>
<section id="same-padding">
<h4>Same Padding<a class="headerlink" href="#same-padding" title="Link to this heading">#</a></h4>
<p>Same padding is like adding extra dough around your cookie sheet so that every part of it can be cut into cookies. In CNNs, this means adding zeros (or other values) around the edges of an image so that the output has the same size as the input after processing. This is particularly useful when you want to preserve the original dimensions of an image.</p>
<p>Real-life analogy: Imagine framing a photo with a mat board. If your photo doesn’t fit perfectly in a frame, you add extra material around it to make it fit neatly.</p>
</section>
<section id="types-of-padding">
<h4>Types of Padding<a class="headerlink" href="#types-of-padding" title="Link to this heading">#</a></h4>
<p>There are different ways to add padding:</p>
<ul class="simple">
<li><p><strong>Zero Padding</strong>: Adds zeros around the edges of an image.</p></li>
<li><p><strong>Reflect Padding</strong>: Reflects the border pixels outward (like a mirror).</p></li>
<li><p><strong>Replicate Padding</strong>: Copies the edge pixels outward.</p></li>
</ul>
<p>Each type has its own use depending on what kind of information you want to preserve at the edges.</p>
<p>Real-life analogy for Zero Padding: It’s like putting blank paper around a drawing to make it bigger.
Real-life analogy for Reflect Padding: It’s like placing a mirror next to your drawing to reflect its edge.
Real-life analogy for Replicate Padding: It’s like stretching out the last part of your drawing to fill extra space.</p>
</section>
<section id="impact-on-output">
<h4>Impact on Output<a class="headerlink" href="#impact-on-output" title="Link to this heading">#</a></h4>
<p>Padding directly impacts how much information is retained in your output. Without padding (valid padding), you lose some details near the edges because they are excluded from processing. With same padding or other types, you preserve more details by making sure every part of the input contributes to the output. This choice affects how well your CNN learns patterns in data, especially when edge information is important.</p>
<p>Real-life analogy: Imagine trying to read text printed on paper where some words at the margins are cut off. Without padding, you miss those words entirely. With padding, it’s like adding blank margins so all words are visible and readable.</p>
</section>
</section>
</section>
<section id="id13">
<h2>Pooling and Padding<a class="headerlink" href="#id13" title="Link to this heading">#</a></h2>
<section id="id14">
<h3>Dimensionality<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Input Size Changes</strong><br />
Imagine you have a large photo, like a family portrait. When you zoom into a specific part of the photo, such as a person’s face, you’re focusing on a smaller section of the image. In Convolutional Neural Networks (CNNs), the input size refers to the entire original photo. As the network processes this image, it might “zoom in” or “crop” parts of it to focus on specific features. This means the size of the input (the whole image) can change as it moves through layers of the network.</p></li>
<li><p><strong>Feature Map Size</strong><br />
Think of feature maps like filters applied to your photo. Imagine putting a grid over your family portrait and looking at each square in detail. Each square might highlight something different: one might show colors, another might show edges, and so on. The feature map is essentially this grid that captures specific details from the image. As you move through layers in the CNN, these grids can shrink or stay the same size depending on how much detail you want to keep.</p></li>
<li><p><strong>Output Dimensions</strong><br />
After processing an image through layers of convolution and pooling, you end up with an output that summarizes all the important features of the image. If we go back to our family portrait analogy, this would be like creating a list of key characteristics about each person in the photo (e.g., hair color, eye shape). The dimensions of this output depend on how much information you’ve extracted and how much you’ve compressed it during processing.</p></li>
<li><p><strong>Size Calculations</strong><br />
To understand size changes in CNNs, think about cutting a piece of paper into smaller sections. If you cut it into equal squares and then remove some edges (like cropping), the size gets smaller. Similarly, in CNNs, operations like pooling or applying filters reduce the size of images step by step. For example, if you have a 10x10 grid and apply a filter that looks at 3x3 sections, your new grid might only be 8x8 because you’re losing some edges during processing.</p></li>
</ul>
</section>
</section>
<section id="id15">
<h2>Transfer Learning<a class="headerlink" href="#id15" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<section id="id16">
<h3>Pre-trained Models<a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Popular Architectures</strong><br />
Imagine you want to build a house, but instead of starting from scratch, you use a pre-built foundation and framework. This is similar to using popular architectures in transfer learning. These architectures, like ResNet, VGG, or Inception, are like tried-and-tested blueprints for neural networks that have been designed by experts and proven to work well on large datasets. They save you time and effort because you don’t have to design everything yourself.</p></li>
<li><p><strong>Model Zoo</strong><br />
Think of a model zoo as a library of ready-made tools. It’s like walking into a hardware store where you can pick the exact tool you need for a specific task. In machine learning, the model zoo is a collection of pre-trained models that are available for download. These models have already been trained on massive datasets like ImageNet and are ready to be adapted for your project.</p></li>
<li><p><strong>Weight Transfer</strong><br />
Imagine learning how to ride a bicycle. Once you’ve mastered it, that skill can help you learn to ride a motorcycle because some of the balance and coordination skills transfer over. Similarly, in transfer learning, weight transfer means taking the knowledge (weights) a model has already learned from one task (like identifying cats and dogs) and applying it to another task (like identifying cars and trucks). This way, the model doesn’t have to start learning from zero.</p></li>
<li><p><strong>Fine-tuning</strong><br />
Fine-tuning is like customizing a suit that’s almost perfect but needs slight adjustments to fit you perfectly. When you take a pre-trained model and adapt it to your specific problem, you’re fine-tuning it. For example, if a model was trained to recognize animals but you want it to recognize specific breeds of dogs, you’ll tweak its parameters slightly so it performs better on your unique dataset.</p></li>
</ul>
</section>
<section id="id17">
<h3>Implementation<a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Feature Extraction</strong><br />
Imagine you are building a new house, but instead of starting from scratch, you decide to reuse the foundation and walls from an older house. In transfer learning, feature extraction works similarly. A pre-trained model (like a neural network trained on a large dataset) has already learned to detect basic patterns or “features” like edges, shapes, or textures. These features are like the foundation and walls of the house—they are reused for a new task. For example, if the pre-trained model learned to recognize animals, you can use those features to help recognize different types of vehicles without starting over.</p></li>
<li><p><strong>Layer Freezing</strong><br />
Think of layer freezing like locking certain parts of your house so they can’t be changed. In a pre-trained model, some layers have already learned useful patterns that don’t need adjustment. By freezing these layers, we ensure they stay as they are while we train only the remaining layers on new data. For instance, if you’re reusing a model trained to recognize cats and dogs but now want it to recognize birds, you might freeze the earlier layers (which detect basic shapes) and only train the later layers (which specialize in identifying specific objects).</p></li>
<li><p><strong>Model Adaptation</strong><br />
Imagine you move into a house that was designed for someone else. You might repaint the walls or change the furniture to suit your needs. Similarly, in model adaptation, we take a pre-trained model and make small changes to its structure or parameters so it fits our new task better. For example, if the original model was trained to classify 10 categories of animals and your task involves 5 categories of flowers, you might replace the final layer with one that outputs predictions for flowers instead.</p></li>
<li><p><strong>Training Strategies</strong><br />
Training strategies in transfer learning are like deciding how much effort you put into renovating your house. Sometimes you only need minor tweaks (fine-tuning), while other times you may need more extensive changes (retraining). For example:</p>
<ul>
<li><p>If your new task is very similar to the old one (like recognizing different breeds of dogs instead of just dogs), you might only fine-tune the last few layers.</p></li>
<li><p>If your task is very different (like identifying medical images instead of animals), you might retrain more layers or even start with a simpler pre-trained model and build upon it.</p></li>
</ul>
</li>
</ul>
</section>
<section id="id18">
<h3>Best Practices<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<p><strong>When to Use</strong></p>
<p>Transfer learning is particularly useful when you don’t have a large dataset to train a model from scratch. Imagine you’re trying to teach a child to recognize different types of birds. Instead of starting from zero, you might first show them a book about animals, which includes birds. This way, they already have some basic understanding of what birds are. Similarly, in transfer learning, you take a model that has been pre-trained on a large dataset (like recognizing animals) and fine-tune it for your specific task (like recognizing specific bird species). This approach is beneficial when data is scarce or when you want to save time and computational resources.</p>
<p><strong>Model Selection</strong></p>
<p>Choosing the right pre-trained model is like picking the right tool for a job. If you need to hang a picture, you’d choose a hammer over a screwdriver because it’s more suited for driving nails into the wall. Similarly, when selecting a model for transfer learning, consider the task it was originally trained on and how closely it aligns with your own task. For example, if you’re working on image classification, models pre-trained on large image datasets like ImageNet are often good choices because they already understand various visual features.</p>
<p><strong>Performance Tips</strong></p>
<p>To get the best performance out of transfer learning, think of it like customizing a suit. You start with a suit that fits reasonably well (the pre-trained model) and then make adjustments so it fits perfectly (fine-tuning). Begin by freezing the early layers of the network—these layers usually capture general features like edges and textures that are useful across many tasks. Then, focus on training the later layers that are more specific to your task. Additionally, adjusting hyperparameters such as learning rate can significantly impact performance. It’s akin to fine-tuning the engine of a car to ensure it runs smoothly under different conditions.</p>
<p><strong>Common Pitfalls</strong></p>
<p>One common mistake in transfer learning is overfitting, which is like cramming for an exam but forgetting everything afterward because you only memorized facts without understanding them. This happens when the model learns too much from your small dataset and doesn’t generalize well to new data. To avoid this, ensure you have sufficient regularization techniques in place and validate your model’s performance on unseen data frequently. Another pitfall is using a pre-trained model that isn’t suitable for your task—like using a butter knife to cut steak, it just won’t work well. Always assess whether the features learned by the pre-trained model are relevant to your problem domain before proceeding with transfer learning.</p>
</section>
</section>
<section id="id19">
<h2>Computer Vision Applications<a class="headerlink" href="#id19" title="Link to this heading">#</a></h2>
<hr class="docutils" />
<section id="id20">
<h3>Image Classification<a class="headerlink" href="#id20" title="Link to this heading">#</a></h3>
<p>Image classification is a fundamental task in computer vision where the goal is to assign a label to an image. Imagine you are sorting a box of mixed fruit into separate baskets. Each basket represents a different type of fruit, such as apples, oranges, or bananas. Similarly, in image classification, each label represents a different category that an image might belong to.</p>
<ul class="simple">
<li><p><strong>Single Label</strong>: This is like sorting fruits where each fruit can only belong to one basket. For example, an image of a cat would be classified solely as “cat.” In this scenario, the model assigns one label to each image.</p></li>
<li><p><strong>Multi-Label</strong>: Imagine you have a fruit salad that contains multiple types of fruits. Here, each piece can belong to more than one category. In multi-label classification, an image might have several labels. For example, a photograph of a beach scene might be labeled as “beach,” “sunset,” and “vacation.”</p></li>
<li><p><strong>Hierarchical Classification</strong>: Think of organizing your fruits not just by type but also by broader categories like “citrus” or “berries.” Hierarchical classification involves categorizing images at multiple levels. An image of a tiger could first be classified under “animal,” then “mammal,” and finally more specifically as “tiger.”</p></li>
<li><p><strong>Real-world Examples</strong>: Consider using your smartphone’s camera app that can identify objects in real-time. When you point it at a flower, it might tell you it’s a “rose.” This is single-label classification. If it recognizes both the flower and the bee on it, that’s multi-label classification. In hierarchical classification, it might first identify the scene as “nature” before specifying the objects within it.</p></li>
</ul>
<p>These examples illustrate how convolutional neural networks (CNNs) help computers see and understand images much like humans do, enabling applications such as facial recognition, autonomous vehicles, and medical imaging diagnostics.</p>
</section>
<section id="id21">
<h3>Object Detection<a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
<section id="bounding-boxes">
<h4>Bounding Boxes<a class="headerlink" href="#bounding-boxes" title="Link to this heading">#</a></h4>
<p>Imagine you’re a photographer, and you’re trying to capture a picture of a bird in a tree. You want to draw a rectangle around the bird in the photo to highlight it for someone else. This rectangle is what we call a “bounding box.” In object detection, bounding boxes are used to mark the location of objects in an image. For example, if you have a picture of a street with cars, people, and traffic lights, bounding boxes would outline each car, person, and traffic light so that we know exactly where they are in the image.</p>
<p>Bounding boxes are like drawing an imaginary frame around something you want to focus on. They help computers understand <em>where</em> an object is located within an image. Think of it as teaching a computer to play “I spy” by pointing out objects in pictures with these frames.</p>
</section>
<section id="object-localization">
<h4>Object Localization<a class="headerlink" href="#object-localization" title="Link to this heading">#</a></h4>
<p>Now let’s say you’re not just identifying objects but also trying to pinpoint <em>exactly where</em> they are in the image. This is what object localization does. It’s not just about saying “there’s a cat in this picture,” but also identifying its position—like saying, “the cat is sitting in the top-left corner.”</p>
<p>An analogy would be using a treasure map. The map tells you there’s treasure (object detection), but it also gives you the exact coordinates or location of the treasure (object localization). Computers use this concept to figure out precisely where objects are within an image.</p>
</section>
<section id="multiple-objects">
<h4>Multiple Objects<a class="headerlink" href="#multiple-objects" title="Link to this heading">#</a></h4>
<p>What happens when there’s more than one object in an image? For example, imagine looking at a photo of a fruit basket with apples, bananas, and oranges. Multiple objects mean the computer has to detect and locate <em>all</em> the different items in the same image. It’s like being at a crowded party and trying to recognize everyone you know—each person is their own “object,” and you need to identify all of them.</p>
<p>In object detection, handling multiple objects means creating separate bounding boxes for each item and identifying what each box contains. It’s like labeling every fruit in that basket with its name while also marking its location.</p>
</section>
<section id="detection-networks">
<h4>Detection Networks<a class="headerlink" href="#detection-networks" title="Link to this heading">#</a></h4>
<p>Detection networks are like specialized tools designed to do all this work efficiently. Imagine you’re assembling furniture from a box—you might use specific tools like screwdrivers or wrenches to get the job done faster and more accurately. Similarly, detection networks are pre-designed systems that help computers detect and localize objects within images.</p>
<p>Some popular detection networks include YOLO (You Only Look Once) and Faster R-CNN. These networks are trained to quickly scan images and identify multiple objects at once. Think of them as highly skilled workers who can look at a photo and instantly tell you everything about what’s in it and where it is!</p>
</section>
</section>
<section id="face-recognition">
<h3>Face Recognition<a class="headerlink" href="#face-recognition" title="Link to this heading">#</a></h3>
<p>Imagine you are at a party with a lot of people. You might not know everyone, but you can recognize your friends and family members even if they are wearing different clothes or have changed their hairstyle. This is similar to how face recognition works in computer vision. It involves identifying or verifying a person from a digital image or video by analyzing the unique features of their face, such as the distance between the eyes, the shape of the nose, and the contour of the lips.</p>
<p>In real life, think of face recognition like a very smart doorman at an exclusive club. This doorman has an incredible memory and can remember every guest’s face. When someone approaches, the doorman quickly checks if their face matches any of the faces in his memory before letting them in. Similarly, face recognition systems compare the features of a face in an image to those stored in a database to determine if there is a match.</p>
</section>
<section id="image-segmentation">
<h3>Image Segmentation<a class="headerlink" href="#image-segmentation" title="Link to this heading">#</a></h3>
<p>Consider a coloring book where each page has different shapes outlined but not colored in. Image segmentation is like taking that coloring book and dividing each page into sections based on those outlines so that you can fill them with different colors. In computer vision, image segmentation involves partitioning an image into multiple segments or regions to simplify its representation and make it more meaningful for analysis.</p>
<p>Imagine you are sorting a box of mixed candies by type. You separate chocolates from gummies and hard candies into different piles. Image segmentation does something similar by separating different objects within an image so that each one can be analyzed individually. For example, in a photo of a dog in a park, segmentation would help identify which pixels belong to the dog and which belong to the background.</p>
</section>
<section id="style-transfer">
<h3>Style Transfer<a class="headerlink" href="#style-transfer" title="Link to this heading">#</a></h3>
<p>Think of style transfer like being able to paint your house with the same style as your favorite artist’s painting without actually changing the structure of your house. In computer vision, style transfer involves taking the artistic style of one image and applying it to another image while keeping its original content intact.</p>
<p>Imagine you have a photo of your pet dog and you love Vincent van Gogh’s “Starry Night.” Style transfer would allow you to transform your dog’s photo so that it looks like it was painted by van Gogh, complete with swirling skies and vibrant colors, while still clearly depicting your dog as the subject.</p>
</section>
<section id="video-processing">
<h3>Video Processing<a class="headerlink" href="#video-processing" title="Link to this heading">#</a></h3>
<p>Video processing is like editing a movie where you can cut scenes, add special effects, or adjust colors to enhance the storytelling. It involves analyzing and manipulating video frames to extract useful information or improve visual quality.</p>
<p>Consider watching a sports game on TV where instant replays show critical moments from different angles in slow motion. Video processing makes this possible by breaking down the video into individual frames, allowing editors to highlight specific actions or apply effects that enhance the viewing experience. Similarly, in computer vision, video processing can be used for tasks like tracking moving objects or detecting unusual activities in surveillance footage.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Chapter%206%20-%20Deep%20Learning%20Tools.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 6 - Deep Learning Tools</p>
      </div>
    </a>
    <a class="right-next"
       href="Chapter%208%20-%20Sequential%20Data%20and%20RNNs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 8 - Sequential Data and RNNs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Chapter 7 - Convolutional Neural Networks</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Chapter 7: Convolutional Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-processing-basics">Image Processing Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-digital-images">Understanding Digital Images</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-properties">Image Properties</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cnn-architecture">CNN Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-components">Basic Components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-organization">Layer Organization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-concepts">Advanced Concepts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-and-padding">Pooling and Padding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling-methods">Pooling Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#padding-techniques">Padding Techniques</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality">Dimensionality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">Transfer Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-trained-models">Pre-trained Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#best-practices">Best Practices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computer-vision-applications">Computer Vision Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-classification">Image Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#object-detection">Object Detection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-applications">Advanced Applications</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Image Processing Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Understanding Digital Images</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pixels-and-colors">Pixels and Colors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rgb-channels">RGB Channels</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#image-resolution">Image Resolution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#color-depth">Color Depth</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Image Processing Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Image Properties</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-detection">Feature Detection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#edge-recognition">Edge Recognition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pattern-identification">Pattern Identification</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#image-preprocessing">Image Preprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">CNN Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Basic Components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Layer Organization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Advanced Concepts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Pooling and Padding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Pooling Methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Padding Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#valid-padding">Valid Padding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#same-padding">Same Padding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-padding">Types of Padding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#impact-on-output">Impact on Output</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Pooling and Padding</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">Dimensionality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">Transfer Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">Pre-trained Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Best Practices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">Computer Vision Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Image Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">Object Detection</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bounding-boxes">Bounding Boxes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#object-localization">Object Localization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-objects">Multiple Objects</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#detection-networks">Detection Networks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#face-recognition">Face Recognition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-segmentation">Image Segmentation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#style-transfer">Style Transfer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#video-processing">Video Processing</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Shreyash Gupta
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>